{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALU_Net.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XijwVpR4s0sQ"
      },
      "source": [
        "# A neural net that tries to become an ALU (arithmetic logic unit)\n",
        "\n",
        "This notebook can run\n",
        "\n",
        "- on local jupyter instances with a local graphics card\n",
        "- on Mac M1 with local jupyter instance and [Apple's tensorflow-plugin](https://developer.apple.com/metal/tensorflow-plugin/)\n",
        "- on Google Colab instances with either GPU or TPU runtime. The colab version uses a Google Drive account to cache data and model state within a Google Drive directory `My Drive/Colab Notebooks/ALU_Net`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ui7VokTJqc"
      },
      "source": [
        "## 1. Configuration and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2sNM2TTJqm"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "use_keras_project_versions=False\n",
        "# Namespaces, namespaces\n",
        "if use_keras_project_versions is False:\n",
        "    # print(\"Importing Keras from tensorflow project (it won't work otherwise with TPU)\")\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "else:\n",
        "    # print(\"Importing Keras from keras project (which had recently declared independence [again]) -- as recommended\")\n",
        "    use_keras_project_versions=True\n",
        "    import keras\n",
        "    from keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "\n",
        "try:\n",
        "    # Google Drive is used in Colab instances to save trained nets and tensorboard logs\n",
        "    from google.colab import drive\n",
        "    is_colab_init = True\n",
        "except:\n",
        "    is_colab_init = False\n",
        "    pass\n",
        "\n",
        "if is_colab_init is True:\n",
        "    # The following code loads the utility module ALU_Tools.py directly from github\n",
        "    # Into Google Colab (or other jupyter instances)\n",
        "    def import_from_github(fn, repo_link, force_github_update=False):\n",
        "        if os.path.exists(fn) is False or force_github_update is True:\n",
        "            print(f\"Loading {fn} module from github...\")\n",
        "            if os.path.exists(fn) is True:\n",
        "                !rm -v {fn}\n",
        "            !wget {repo_link}\n",
        "    force_github_update = True  # Note: Even if set to True, you still need to restart the runtime to get an updated version.\n",
        "    import_from_github('ml_env_tools.py','https://raw.githubusercontent.com/domschl/ALU_Net/main/ml_env_tools.py',force_github_update)\n",
        "    import_from_github('ALU_Dataset.py','https://raw.githubusercontent.com/domschl/ALU_Net/main/ALU_Dataset.py',force_github_update)\n",
        "\n",
        "from ml_env_tools import MLEnv\n",
        "from ALU_Dataset import ALU_Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Ag-07-gmWM"
      },
      "source": [
        "def model_large(inputs, regu1=1e-7, regu2=1e-7, regu3=1e-7, neurons=1024, filters=16, strides=2, kernel_size=3):  # neurons must be divisible by 4 for the residual below\n",
        "    # Input goes parallel into 3 streams which will be combined at the end:\n",
        "    # Stream 1: convolutions\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)  # x0)\n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x1 = d1(rinp)\n",
        "    filters=filters*2\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x2 = d2(x1)\n",
        "    filters=filters*2\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x3 = d3(x2)\n",
        "    filters=filters*2\n",
        "    d4 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xcvl = d4(x3)\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(xcvl)\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    # Stream 2: simple dense layers\n",
        "    df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf1 = df1(inputs)\n",
        "    df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf2 = df2(xf1)\n",
        "    df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf3 = df3(xf2)\n",
        "\n",
        "    # Stream3: dense layers with residual pathway\n",
        "    dfa1 = layers.Dense(neurons/4, kernel_regularizer=regularizers.l2(\n",
        "        regu3), activation=\"relu\")\n",
        "    xfa1 = dfa1(inputs)\n",
        "    dfa2 = layers.Dense(neurons/4, kernel_regularizer=regularizers.l2(\n",
        "        regu3), activation=\"relu\")\n",
        "    con0 = layers.Concatenate()\n",
        "    xfa2 = con0([dfa2(xfa1),xfa1])\n",
        "    dfa3 = layers.Dense(neurons/2, kernel_regularizer=regularizers.l2(regu3), activation=\"relu\")\n",
        "    con1 = layers.Concatenate()\n",
        "    xfa3 = con1([dfa3(xfa2),xfa2])\n",
        "\n",
        "    # Concat of stream1,2,3\n",
        "    con = layers.Concatenate()\n",
        "    xcon = con([xe1, xf3, xfa3])\n",
        "    dc1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xc1 = dc1(xcon)\n",
        "\n",
        "    # Use sigmoid to map to bits 0..1\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xc1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fThFyT8DgmWN"
      },
      "source": [
        "def model_medium(inputs, regu1=1e-7, regu2=1e-8, neurons=256, lstm_neurons=256, filters=64, kernel_size=3, strides=2):\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs) # xf1)\n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x1 = d1(rinp)\n",
        "    filters = 2*filters\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x2 = d2(x1)\n",
        "    filters = 2*filters\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x3 = d3(x2)\n",
        "\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr1 = r1(rinp)\n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr2 = r2(xr1)\n",
        "    r3 = layers.LSTM(neurons, return_sequences=True)\n",
        "    xr3 = r3(xr2)\n",
        "\n",
        "    x3t = tf.transpose(x3,[0,2,1])\n",
        "    xr3t = tf.transpose(xr3,[0,2,1])\n",
        "\n",
        "    cc = layers.Concatenate()\n",
        "    xc = cc([x3t, xr3t])\n",
        "\n",
        "    dcc = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xcc = dcc(xc)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(xcc)\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7QEXpUIgmWN"
      },
      "source": [
        "def model_minimal(inputs, neurons=1280, regu1=1e-8):\n",
        "    df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf1 = df1(inputs)\n",
        "    df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf2s = df2(xf1)\n",
        "    dc1 = layers.Concatenate()\n",
        "    xf2 = dc1([xf2s, xf1])\n",
        "    \n",
        "    df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf3s = df3(xf2)\n",
        "    dc2 = layers.Concatenate()\n",
        "    xf3 = dc2([xf3s, xf2])\n",
        "    \n",
        "    df4 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf4s = df4(xf3)\n",
        "    dc3 = layers.Concatenate()\n",
        "    xf4 = dc3([xf4s, xf3])\n",
        "\n",
        "    df5 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf5s = df5(xf4)\n",
        "    dc4 = layers.Concatenate()\n",
        "    xf5 = dc4([xf5s, xf4])\n",
        "\n",
        "    df6 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf6s = df6(xf5)\n",
        "    dc5 = layers.Concatenate()\n",
        "    xf6 = dc5([xf6s, xf5])\n",
        "\n",
        "    df7 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf7s = df7(xf6)\n",
        "    dc6 = layers.Concatenate()\n",
        "    xf7 = dc6([xf7s, xf6])\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xf7)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltKUec3ahL8g"
      },
      "source": [
        "def model_minimal_prm(inputs, params):\n",
        "    df=[]\n",
        "    xf=[]\n",
        "    xfs=[]\n",
        "    dc=[]\n",
        "    df.append(layers.Dense(params[\"neurons\"], kernel_regularizer=regularizers.l2(\n",
        "        params[\"regu1\"]), activation=\"relu\"))\n",
        "    xf.append(df[0](inputs))\n",
        "\n",
        "    for layer in range(1,params[\"layer_cnt\"]-1):\n",
        "        df.append(layers.Dense(params[\"neurons\"], kernel_regularizer=regularizers.l2(\n",
        "            params[\"regu1\"]), activation=\"relu\"))\n",
        "        xfs.append(df[layer](xf[layer-1]))\n",
        "        dc.append(layers.Concatenate())\n",
        "        xf.append(dc[layer-1]([xfs[layer-1], xf[layer-1]]))\n",
        "    print(f\"len xf: {len(xf)}, {params['layer_cnt']}\")\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xf[params[\"layer_cnt\"]-2])\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8NH4aMnkzKL"
      },
      "source": [
        "# model_minimal_prm(np.random.randn(*[2,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL9cJR-NgmWN"
      },
      "source": [
        "\n",
        "def model_minimal_recurrent(inputs, neurons=64, lstm_neurons=128, regu1=1e-7):\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr1 = r1(rinp)\n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr2 = r2(xr1)\n",
        "    r3 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr3 = r3(xr2)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(xr3)\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4jmpsD2gmWO"
      },
      "source": [
        "def model_conv1d_recurrent(inputs, neurons=512, lstm_neurons=386, filters=128, kernel_size=3, strides=2, regu0=1e-7, regu1=1e-7, regu2=1e-7):\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)\n",
        "\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr1 = r1(rinp)\n",
        "    \n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x1 = d1(xr1)\n",
        "    \n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr2 = r2(x1)\n",
        "\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(regu2), activation=\"relu\")\n",
        "    x2 = d2(xr2)\n",
        "\n",
        "    r3 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr3 = r3(x2)\n",
        "\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(regu2), activation=\"relu\")\n",
        "    x3 = d3(xr3)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(x3)\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn3rAlvqgmWO"
      },
      "source": [
        "def create_load_model(model_variant, params, save_path=None):\n",
        "    \"\"\" Create or load a model \"\"\"\n",
        "    if save_path is None or not os.path.exists(save_path): #or is_tpu is True:\n",
        "        print(\"Initializing new model...\")\n",
        "        inputs = keras.Input(shape=(36,))  # depends on encoding of op-code!\n",
        "        if model_variant not in model_variants:\n",
        "            print('Unkown model type')\n",
        "            return None\n",
        "        outputs = model_variants[model_variant](inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths_\"+model_variant)\n",
        "        print(f\"Compiling new model of type {model_variant}\")\n",
        "        if use_keras_project_versions is False: \n",
        "            opti = keras.optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
        "        else:\n",
        "            opti = optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "    else:\n",
        "        print(f\"Loading standard-format model of type {model_variant} from {model_save_dir}\")\n",
        "        model = tf.keras.models.load_model(save_path)\n",
        "        print(\"Continuing training from existing model\")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqxHzunzgmWP"
      },
      "source": [
        "def get_model(ml_env, model_variant, params, save_path=None, on_tpu=False, import_weights=False):\n",
        "    if on_tpu is True:\n",
        "        if ml_env.tpu_is_init is False:\n",
        "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=ml_env.tpu_address)\n",
        "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
        "            ml_env.tpu_is_init=True\n",
        "        with tpu_strategy.scope():\n",
        "            print(\"Creating TPU-scope model\")\n",
        "            model = create_load_model(model_variant, params, save_path=save_path)\n",
        "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
        "            print(\"Injecting saved weights into TPU model, loading...\")\n",
        "            temp_model = create_load_model(model_variant, params, save_path=save_path)\n",
        "            temp_model.load_weights(ml_env.weights_file)\n",
        "            print(\"Injecting...\")\n",
        "            model.set_weights(temp_model.get_weights())\n",
        "            print(\"Updated TPU weights from saved model\")\n",
        "        return model\n",
        "    else:\n",
        "        print(\"Creating standard-scope model\")\n",
        "        model = create_load_model(model_variant, params, save_path=save_path)\n",
        "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
        "            print(\"Injecting saved weights into model, loading...\")        \n",
        "            model.load_weights(ml_env.weights_file)\n",
        "            imported_weights_file = ml_env.weights_file+'-imported'\n",
        "            os.rename(ml_env.weights_file, imported_weights_file)\n",
        "            print(f\"Renamed weights file {ml_env.weights_file} to {imported_weights_file} to prevent further imports!\")\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4HDT65wgmWP"
      },
      "source": [
        "def math_train(mlenv:MLEnv, model, dataset, validation, batch_size=8192, epochs=5000, steps_per_epoch=2000, log_path=\"./logs\"):\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    interrupted = 2\n",
        "    tensorboard_callback = callbacks.TensorBoard(\n",
        "        log_dir=log_path\n",
        "        # histogram_freq=1\n",
        "        # update_freq='batch'\n",
        "        )\n",
        "    if mlenv.is_tpu is False: # TPUs update Tensorboard too asynchronously, data is corrupted by updates during mirroring.\n",
        "        lambda_callback = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_end = ml_env.epoch_time_func\n",
        "        )\n",
        "    try:\n",
        "        if ml_env.is_tpu:\n",
        "            model.fit(dataset, validation_data=validation, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[tensorboard_callback])\n",
        "            interrupted=0\n",
        "        else:\n",
        "            model.fit(dataset, validation_data=validation, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[tensorboard_callback, lambda_callback])\n",
        "            interrupted=0\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"---------INTERRUPT----------\")\n",
        "        print(\"\")\n",
        "        print(\"Training interrupted\")\n",
        "        interrupted = 1 # user stopped runtime\n",
        "    except Exception as e:\n",
        "        interruped = 2  # Bad: something crashed.\n",
        "        print(f\"INTERNAL ERROR\")\n",
        "        print(f\"Exception {e}\")\n",
        "    finally:\n",
        "        return interrupted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9_Ey8nDgmWP"
      },
      "source": [
        "def instantiate_models(ml_env:MLEnv, model_variant, params, save_path=None, import_weights=True):\n",
        "    if ml_env.is_tpu:\n",
        "        # Generate a second CPU model for testing:\n",
        "        test_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
        "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=True, import_weights=import_weights)\n",
        "    else:\n",
        "        test_model = None\n",
        "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
        "    return math_model, test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL4L7FZYgmWP"
      },
      "source": [
        "def do_training(mlenv:MLEnv, math_model, training_dataset, validation_dataset, math_data, epochs_per_cycle, model_save_dir=None, \n",
        "                weights_file=None, test_model=None, cycles=100, steps_per_epoch=1000, reweight_size=1000, valid_ops=None, regenerate_data_after_cycles=0, data_func=None,\n",
        "                log_path='./logs'):\n",
        "    # Training\n",
        "    for mep in range(0, cycles):\n",
        "        print()\n",
        "        print()\n",
        "        print(f\"------ Meta-Epoch {mep+1}/{cycles} ------\")\n",
        "        print()\n",
        "        if regenerate_data_after_cycles!=0 and data_func is not None:\n",
        "            if mep>0 and (mep+1)%regenerate_data_after_cycles==0:\n",
        "                training_dataset, validation_dataset = data_func()\n",
        "        if mep==0 and ml_env.is_tpu is True:\n",
        "            print(\"There will be some warnings by Tensorflow, documenting some state of internal decoherence, currently they can be ignored.\")\n",
        "        interrupted = math_train(ml_env, math_model, training_dataset, validation=validation_dataset, epochs=epochs_per_cycle, steps_per_epoch=steps_per_epoch, log_path=log_path)\n",
        "        if interrupted <2:\n",
        "            if ml_env.is_tpu:\n",
        "                mlenv.gdrive_log_mirror()  # TPUs can only savely mirror Tensorboard data once training is finished for an meta-epoch.\n",
        "                if test_model is None:\n",
        "                    print(\"Fatal: tpu-mode needs test_model on CPU\")\n",
        "                    return False\n",
        "                print(\"Injecting weights into test_model:\")\n",
        "                test_model.set_weights(math_model.get_weights())\n",
        "                if weights_file is not None:\n",
        "                    print(f\"Saving test-model weights to {weights_file}\")\n",
        "                    test_model.save_weights(weights_file)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(test_model, samples=reweight_size, short_math=False, valid_ops=valid_ops, verbose=False)\n",
        "            else:\n",
        "                if model_save_dir is not None:\n",
        "                    print(\"Saving math-model\")\n",
        "                    math_model.save(model_save_dir)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(math_model, samples=reweight_size, short_math=False, valid_ops=valid_ops, verbose=False)\n",
        "        if interrupted>0:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yjwuceDgmWQ"
      },
      "source": [
        "model_variants = {\"large\": model_large,\n",
        "                  \"medium\": model_medium,\n",
        "                  \"minimal\": model_minimal,\n",
        "                  \"minimal_prm\": model_minimal_prm,\n",
        "                  \"minimal_recurrent\": model_minimal_recurrent,\n",
        "                  \"conv_recurrent\": model_conv1d_recurrent\n",
        "            }\n",
        "\n",
        "model_variant = 'minimal_prm'  # see: model_variants definition.\n",
        "epochs_per_cycle = 100\n",
        "cycles = 1  # perform 100 cycles, each cycle trains with epochs_per_cycle epochs.\n",
        "regenerate_data_after_cycles = 0  # if !=0, the training data will be created anew after each number of \n",
        "                                  # regenerace_data_after_cycles cycles. Disadvantage: when training TPU, \n",
        "                                  # Google might use the time it takes to regenerate to training data to \n",
        "                                  # terminate your session :-/\n",
        "samples = 200000  # Number training data examples\n",
        "batch_size = 2000\n",
        "learning_rate = 0.001\n",
        "valid_ops = None  # Default: None (all ops), or list of ops, e.g. ['*', '/'] trains only multiplication and division.\n",
        "steps_per_epoch = samples // batch_size  # TPU stuff\n",
        "\n",
        "params={\n",
        "    \"layer_cnt\": 4,\n",
        "    \"neurons\":128, \n",
        "    \"learning_rate\": 0.001,\n",
        "    \"regu1\": 1e-8\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnBLx9GwgmWQ"
      },
      "source": [
        "ml_env=MLEnv()\n",
        "math_data=ALU_Dataset(ml_env)\n",
        "\n",
        "model_save_dir, weights_file, cache_stub, log_path = ml_env.init_paths(\"ALU_Net\", \"math_model\", model_variant=model_variant, log_to_gdrive=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wN1G_qPgmWQ"
      },
      "source": [
        "create_train_val_data = lambda regen : math_data.get_datasets(pre_weight=True, samples=samples, validation_samples=50000, batch_size=batch_size, short_math=False, \n",
        "                                     valid_ops=valid_ops, cache_file_stub=cache_stub, use_cache=True, regenerate_cached_data=regen)\n",
        "create_train_val_data_regen = lambda : create_train_val_data(True)\n",
        "train, val = create_train_val_data(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSL6lysogmWQ"
      },
      "source": [
        "math_model, test_model = instantiate_models(ml_env, model_variant, params, model_save_dir, import_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epfwj4czgmWQ"
      },
      "source": [
        "try:\n",
        "    # use the python variable log_path:\n",
        "    get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntW3khTWgmWQ"
      },
      "source": [
        "do_training(ml_env, math_model, train, val, math_data, epochs_per_cycle, model_save_dir=model_save_dir, \n",
        "            weights_file=weights_file, test_model=test_model, cycles=cycles, steps_per_epoch=steps_per_epoch, valid_ops=valid_ops, \n",
        "            regenerate_data_after_cycles=regenerate_data_after_cycles, data_func=create_train_val_data_regen, log_path=log_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO6dRCu6TJqm"
      },
      "source": [
        "# Testing and applying the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcGr9h_p9_R",
        "tags": []
      },
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    test_model = math_model\n",
        "math_data.check_results(test_model, samples=100, short_math=False, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv8ZRa8GpThA"
      },
      "source": [
        "dx,dy,_,_,_=math_data.create_data_point(22,33,'+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZt0CbNdqpqW"
      },
      "source": [
        "math_data.decode_results(test_model.predict(np.array([dx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV52DL3gq0rI"
      },
      "source": [
        "def calc(inp):\n",
        "    args=inp.split(' ')\n",
        "    if len(args)!=3:\n",
        "        print(\"need three space separated tokens: <int> <operator> <int>, e.g. '3 + 4' or '4 XOR 5'\")\n",
        "        return False\n",
        "    if args[1] not in math_data.model_ops:\n",
        "        print(f\"{args[1]} is not a known operator.\")\n",
        "        return False\n",
        "    op1=int(args[0])\n",
        "    op2=int(args[2])\n",
        "    dx,dy,_,_,_=math_data.create_data_point(op1, op2, args[1])\n",
        "    ans=math_data.decode_results(test_model.predict(np.array([dx])))\n",
        "    print(f\"{op1} {args[1]} {op2} = {ans[0]}\")\n",
        "    op=f\"{op1} {args[1]} {op2}\"\n",
        "    op=op.replace('AND', '&').replace('XOR','^').replace('=','==').replace('OR','|')\n",
        "    an2=eval(op)\n",
        "    if ans[0]!=an2:\n",
        "        print(\"Error\")\n",
        "        print(bin(ans[0]))\n",
        "        print(bin(an2))\n",
        "    return ans[0],an2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqeW9hlrEEE"
      },
      "source": [
        "calc(\"222 = 223\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jjSQodrH0s"
      },
      "source": [
        "calc(\"8812 = 8812\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoK-LUr-s9IO"
      },
      "source": [
        "calc(\"3 * 4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQNAv4Fs-_w"
      },
      "source": [
        "calc (\"1 AND 3\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}