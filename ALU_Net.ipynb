{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ALU_Net.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XijwVpR4s0sQ"
      },
      "source": [
        "\"\"\" A neural net that tries to become an ALU (arithmetic logic unit) \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv54Aqp9s_gs"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoLypnLqxL4p"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try: # Colab instance?\n",
        "    from google.colab import drive\n",
        "    is_colab = True\n",
        "except: # Not? ignore.\n",
        "    is_colab = False\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfh0Gsn4ISLY"
      },
      "source": [
        "# Hardware check:\n",
        "\n",
        "is_tpu = False\n",
        "is_gpu = False\n",
        "tpu_is_init = False\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hw_list=tf.config.experimental.list_logical_devices(hw)\n",
        "    if len(hw_list)>0:\n",
        "        if hw=='TPU':\n",
        "            is_tpu=True\n",
        "        if hw=='GPU':\n",
        "            is_gpu=True\n",
        "    print(f\"{hw}: {hw_list}\") \n",
        "\n",
        "if is_colab:\n",
        "    if not is_tpu:\n",
        "        try:\n",
        "            TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "            tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "            is_tpu = True\n",
        "            print(f\"TPU available at {TPU_ADDRESS}\")\n",
        "        except:\n",
        "            print(\"No TPU available\")\n",
        "    else:\n",
        "        print(f\"TPU available, already connected to {TPU_ADDRESS}\")\n",
        "\n",
        "if not is_tpu:\n",
        "    if not is_gpu:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "    else:\n",
        "        print(\"GPU available\")\n",
        "else:\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    print(\"TPU: eager execution disabled!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0TucwitE4T"
      },
      "source": [
        "class ALU_Dataset():\n",
        "    \"\"\" Generate training data for all ALU operations \"\"\"\n",
        "    # The ALU takes two integers and applies one of the supported\n",
        "    # model_ops. Eg op1=123, op2=100, op='-' -> result 23\n",
        "    # The net is supposed to learn to 'calculate' the results for\n",
        "    # arbitrary op1, op2 (positive integers, 0..32767) and \n",
        "    # the twelve supported ops \n",
        "\n",
        "    def __init__(self, pre_weight=False):\n",
        "        self.model_ops = [\"+\", \"-\", \"*\", \"/\", \"%\",\n",
        "                          \"AND\", \"OR\", \"XOR\", \">\", \"<\", \"=\", \"!=\"]\n",
        "        self.model_is_boolean = [False, False, False, False, False,\n",
        "                                 False, False, False, True, True, True, True]\n",
        "        # Probabilites for creating a sample for each of the ops, (Will be\n",
        "        # reweighted on checks to generate for samples for 'difficult' ops):\n",
        "        self.model_dis = [10, 10, 10, 10, 10, 10,   10,  10,   10, 10, 10, 10]\n",
        "        model_dis_w = [19, 12, 110, 15, 36, 10, 10, 10, 10, 10, 10, 10]\n",
        "        self.model_funcs = [self.add_smpl, self.diff_smpl, self.mult_smpl,\n",
        "                            self.div_smpl, self.mod_smpl, self.and_smpl,\n",
        "                            self.bor_smpl, self.xor_smpl, self.greater_smpl,\n",
        "                            self.lesser_smpl, self.eq_smpl, self.neq_smpl]\n",
        "        self.bit_count = 15\n",
        "        self.all_bits_one = 0x7fffffff\n",
        "        self.true_vect = self.all_bits_one\n",
        "        self.false_vect = 0\n",
        "        if pre_weight is True:\n",
        "            self.model_dis=model_dis_w\n",
        "\n",
        "    @staticmethod\n",
        "    def int_to_binary_vect(num_int, num_bits=8):\n",
        "        \"\"\" get a binary encoded vector of n of bit-lenght nm \"\"\"\n",
        "        num_vect = np.zeros(num_bits, dtype=np.float32)\n",
        "        for i in range(0, num_bits):\n",
        "            if num_int & (2**i) != 0:\n",
        "                num_vect[i] = 1.0\n",
        "        return num_vect\n",
        "\n",
        "    @staticmethod\n",
        "    def get_random_bits(bits):\n",
        "        \"\"\" get bits random int 0...2**bits-1 \"\"\"\n",
        "        return random.randint(0, 2**bits-1)\n",
        "\n",
        "    def op_string_to_index(self, op_string):\n",
        "        \"\"\" transform op_string (e.g. '+' -> 0) into corresponding index \"\"\"\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            if self.model_ops[i] == op_string:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def get_data_point(self, equal_distrib=False, short_math=False, valid_ops=None):\n",
        "        \"\"\" Get a random example for on ALU operation for training \"\"\"\n",
        "        result = -1\n",
        "        op1 = self.get_random_bits(self.bit_count)\n",
        "        op2 = self.get_random_bits(self.bit_count)\n",
        "        if valid_ops is not None and len(valid_ops)==0:\n",
        "            valid_ops=None\n",
        "        if valid_ops is not None:\n",
        "            if equal_distrib is False:\n",
        "                print(\"Op restriction via valid_ops forces equal_distrib=True\")\n",
        "                equal_distrib=True\n",
        "            for op in valid_ops:\n",
        "                if op not in self.model_ops:\n",
        "                    print(f'Cannot restrict valid_ops to {op}, unknown operation, ignoring all valid_ops')\n",
        "                    valid_ops=None\n",
        "                    break\n",
        "\n",
        "        if equal_distrib or valid_ops is not None:\n",
        "            if valid_ops is None:   \n",
        "                op_index = random.randint(0, len(self.model_ops)-1)\n",
        "            else:\n",
        "                if len(valid_ops)==1:\n",
        "                    op_index=0\n",
        "                else:\n",
        "                    op_index = random.randint(0, len(valid_ops)-1)\n",
        "                op_index=self.model_ops.index(valid_ops[op_index])\n",
        "        else: # make 'difficult' ops more present in training samples:\n",
        "            rx = 0\n",
        "            for md in self.model_dis:\n",
        "                rx += md\n",
        "            rrx = random.randint(0, rx)\n",
        "            rx = 0\n",
        "            op_index = 0\n",
        "            for op_index in range(0, len(self.model_ops)):\n",
        "                rx += self.model_dis[op_index]\n",
        "                if rx > rrx:\n",
        "                    break\n",
        "        return self.encode_op(op1, op2, op_index, short_math)\n",
        "\n",
        "    def generator(self, samples=20000, equal_distrib=False, short_math=False, valid_ops=None):\n",
        "        while True:\n",
        "            x, Y = self.create_training_data(samples=samples, short_math=short_math, valid_ops=valid_ops, verbose=False, title=None)\n",
        "            #x, Y, _, _, _ = self.get_data_point(equal_distrib=equal_distrib, short_math=short_math, valid_ops=valid_ops)\n",
        "            yield x, Y\n",
        "\n",
        "    def encode_op(self, op1, op2, op_index, short_math=False):\n",
        "        \"\"\" turn two ints and operation into training data \"\"\"\n",
        "        op1, op2, result = self.model_funcs[op_index](op1, op2, short_math)\n",
        "        if self.model_is_boolean[op_index] is True:\n",
        "            if result==self.false_vect:\n",
        "                str_result=\"false\"\n",
        "            elif result==self.true_vect:\n",
        "                str_result=\"true\"\n",
        "            else:\n",
        "                str_result=\"undefined\"\n",
        "        else:\n",
        "            str_result=result\n",
        "        sym = f\"{op1}{self.model_ops[op_index]}{op2}={str_result}\"\n",
        "        inp = np.concatenate(\n",
        "            [self.int_to_binary_vect(op1, num_bits=16),\n",
        "             self.int_to_binary_vect(op_index, num_bits=4),\n",
        "             self.int_to_binary_vect(op2, num_bits=16)])\n",
        "        oup = self.int_to_binary_vect(result, num_bits=32)\n",
        "        return inp, oup, result, op_index, sym\n",
        "\n",
        "    @staticmethod\n",
        "    def add_smpl(op1, op2, _):\n",
        "        \"\"\" addition training example \"\"\"\n",
        "        result = op1+op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def diff_smpl(op1, op2, _):\n",
        "        \"\"\" subtraction training example \"\"\"\n",
        "        if op2 > op1:\n",
        "            op2, op1 = op1, op2\n",
        "        result = op1-op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def mult_smpl(op1, op2, short_math=False):\n",
        "        \"\"\" multiplication training example \"\"\"\n",
        "        if short_math:\n",
        "            op1 = op1 % 1000\n",
        "            op2 = op2 % 1000\n",
        "        result = op1*op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def div_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer division training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1//op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def mod_smpl(self, op1, op2, _):\n",
        "        \"\"\" modulo (remainder) training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1 % op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def and_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise AND training example \"\"\"\n",
        "        result = op1 & op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def bor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise OR training example \"\"\"\n",
        "        result = op1 | op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def xor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise XOR training example \"\"\"\n",
        "        result = op1 ^ op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def greater_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation > training example \"\"\"\n",
        "        if op1 > op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def lesser_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation < training example \"\"\"\n",
        "        if op1 < op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def eq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation == training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 == op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def neq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation != training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 != op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def create_data_point(self, op1, op2, op_string):\n",
        "        \"\"\" create training data from given ints op1, op2 and op_string \"\"\"\n",
        "        op_index = self.op_string_to_index(op_string)\n",
        "        if op_index == -1:\n",
        "            print(f\"Invalid operation {op_string}\")\n",
        "            return np.array([]), np.array([]), -1, -1, None\n",
        "        return self.encode_op(op1, op2, op_index)\n",
        "\n",
        "    def create_training_data(self, samples=10000, short_math=False, valid_ops=None, verbose=True, title=None):\n",
        "        \"\"\" create a number of training samples \"\"\"\n",
        "        x, y, _, _, _ = self.get_data_point()\n",
        "        dpx = np.zeros((samples, len(x)), dtype=np.float32)\n",
        "        dpy = np.zeros((samples, len(y)), dtype=np.float32)\n",
        "        if verbose is True:\n",
        "            if title is None:\n",
        "                print(f\"Creating {samples} data points (. = 1000 progress)\")\n",
        "            else:\n",
        "                print(f\"{title}: Creating {samples} data points (. = 1000 progress)\")\n",
        "\n",
        "        for i in range(0, samples):\n",
        "            if verbose is True:\n",
        "                if i%100000 == 0:\n",
        "                    print(f\"{i:>10} \", end=\"\")\n",
        "            if (i+1) % 1000 == 0:\n",
        "                if verbose is True:\n",
        "                    print(\".\", end=\"\")\n",
        "                    sys.stdout.flush()\n",
        "                    if (i+1) % 100000 == 0:\n",
        "                        print()\n",
        "            if valid_ops is None:\n",
        "                x, y, _, _, _ = self.get_data_point(\n",
        "                    equal_distrib=False, short_math=short_math)\n",
        "            else:\n",
        "                x, y, _, _, _ = self.get_data_point(\n",
        "                    equal_distrib=True, short_math=short_math, valid_ops=valid_ops)\n",
        "            dpx[i, :] = x\n",
        "            dpy[i, :] = y\n",
        "        if verbose is True:\n",
        "            print()\n",
        "        return dpx, dpy\n",
        "\n",
        "    def create_dataset(self, samples=10000, batch_size=2000, short_math=False, valid_ops=None, title=None):\n",
        "        x, Y = self.create_training_data(samples=samples, short_math=short_math, valid_ops=valid_ops, title=title)\n",
        "        shuffle_buffer=10000\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((x, Y)).cache()\n",
        "        dataset=dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
        "        if is_tpu is True:\n",
        "            dataset=dataset.repeat() # Mandatory for Keras TPU for now\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "        dataset=dataset.prefetch(-1) # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "        return dataset\n",
        "\n",
        "    def create_dataset_from_generator(self, short_math=False, valid_ops=None):\n",
        "        dataset=tf.data.Dataset.from_generator(\n",
        "            self.generator,\n",
        "            output_signature=(\n",
        "                    tf.TensorSpec(shape=(None,36), dtype=np.float32),\n",
        "                    tf.TensorSpec(shape=(None,32), dtype=np.float32))\n",
        "            )\n",
        "        return dataset\n",
        "        \n",
        "    @staticmethod\n",
        "    def decode_results(result_int_vects):\n",
        "        \"\"\" take an array of 32-float results from neural net and convert to ints \"\"\"\n",
        "        result_vect_ints = []\n",
        "        for vect in result_int_vects:\n",
        "            if (len(vect) != 32):\n",
        "                print(f\"Ignoring unexpected vector of length {len(vect)}\")\n",
        "            else:\n",
        "                int_result = 0\n",
        "                for i in range(0, 32):\n",
        "                    if vect[i] > 0.5:\n",
        "                        int_result += 2**i\n",
        "                result_vect_ints.append(int_result)\n",
        "        return result_vect_ints\n",
        "\n",
        "    def check_results(self, model, samples=1000, short_math=False, valid_ops=None, verbose=False):\n",
        "        \"\"\" Run a number of tests on trained model \"\"\"\n",
        "        ok = 0\n",
        "        err = 0\n",
        "        operr = [0]*len(self.model_ops)\n",
        "        opok = [0]*len(self.model_ops)\n",
        "        for _ in range(0, samples):\n",
        "            x, _, z, op, s = self.get_data_point(\n",
        "                equal_distrib=True, valid_ops=valid_ops, short_math=short_math)\n",
        "            res = self.decode_results(model.predict(np.array([x])))\n",
        "            if res[0] == z:\n",
        "                ok += 1\n",
        "                opok[op] += 1\n",
        "                r = \"OK\"\n",
        "            else:\n",
        "                err += 1\n",
        "                operr[op] += 1\n",
        "                r = \"Error\"\n",
        "            if verbose is True:\n",
        "                if self.model_is_boolean[op] is True:\n",
        "                    if res[0]==self.false_vect:\n",
        "                        str_result=\"false\"\n",
        "                    elif res[0]==self.true_vect:\n",
        "                        str_result=\"true\"\n",
        "                    else:\n",
        "                        str_result=\"undefined\"\n",
        "                else:\n",
        "                    str_result=res[0]\n",
        "                if res[0]==z:\n",
        "                    print(f\"{s} == {str_result}: {r}\")\n",
        "                else:\n",
        "                    print(f\"{s} != {str_result}: {r}\")\n",
        "                    if self.model_is_boolean[op] is False:\n",
        "                        print(bin(res[0]))\n",
        "                        print(bin(z))\n",
        "        opsum = ok+err\n",
        "        if opsum == 0:\n",
        "            opsum = 1\n",
        "        print(f\"Ok: {ok}, Error: {err} -> {ok/opsum*100.0}%\")\n",
        "        print(\"\")\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            opsum = opok[i]+operr[i]\n",
        "            if opsum == 0:\n",
        "                continue\n",
        "            # modify the distribution of training-data generated to favour\n",
        "            # ops with bad test results, so that more training data is\n",
        "            # generated on difficult cases:\n",
        "            self.model_dis[i] = int(operr[i]/opsum*100)+10\n",
        "            print(\n",
        "                f\"OP{self.model_ops[i]}: Ok: {opok[i]}, Error: {operr[i]}\", end=\"\")\n",
        "            print(f\" -> {opok[i]/opsum*100.0}%\")\n",
        "        print(\"Change probability for ops in new training data:\")\n",
        "        print(f\"Ops:    {self.model_ops}\")\n",
        "        print(f\"Weight: {self.model_dis}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUX8j9nk6IWI"
      },
      "source": [
        "DENSE_NEURONS = 1024\n",
        "FILTERS = 128\n",
        "REGULARIZER1 = 1e-8\n",
        "REGULARIZER2 = 1e-8\n",
        "REGULARIZER3 = 1e-7\n",
        "KERNEL_SIZE = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSdpsHyfti96"
      },
      "source": [
        "def create_load_model(save_path=None):\n",
        "    \"\"\" Create of load a model \"\"\"\n",
        "    if save_path is None or not os.path.exists(save_path) or is_tpu is True:\n",
        "        regu1 = REGULARIZER1\n",
        "        regu2 = REGULARIZER2\n",
        "        regu3 = REGULARIZER3\n",
        "        neurons = DENSE_NEURONS\n",
        "        inputs = keras.Input(shape=(36,))  # depends on encoding of op-code!\n",
        "\n",
        "        shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "        rinp = shaper(inputs)  # x0)\n",
        "        d1 = layers.Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x1 = d1(rinp)\n",
        "        d2 = layers.Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x2 = d2(x1)\n",
        "        d3 = layers.Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x3 = d3(x2)\n",
        "        d4 = layers.Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        xcvl = d4(x3)\n",
        "        flatter = layers.Flatten()\n",
        "        xf = flatter(xcvl)\n",
        "        de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xe1 = de1(xf)\n",
        "\n",
        "        df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf1 = df1(inputs)\n",
        "        df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf2 = df2(xf1)\n",
        "        df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf3 = df3(xf2)\n",
        "\n",
        "        dfa1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu3), activation=\"relu\")\n",
        "        xfa1 = dfa1(inputs)\n",
        "        dfa2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu3), activation=\"relu\")\n",
        "        xfa2 = dfa2(xfa1)+xfa1\n",
        "        dfa3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu3), activation=\"relu\")\n",
        "        xfa3 = dfa3(xfa2)+xfa2\n",
        "        \n",
        "        con = layers.Concatenate()\n",
        "        xcon = con([xe1, xf3, xfa3])\n",
        "        dc1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xc1 = dc1(xcon)\n",
        "\n",
        "        de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "        outputs = de2(xc1)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths\")\n",
        "        # , metrics=[\"accuracy\"])\n",
        "        print(\"Compiling new model\")\n",
        "        if is_tpu: # RULE: TPUs need *way* lower learning rates than GPUs!\n",
        "            opti = keras.optimizers.Adam(learning_rate=0.0006)\n",
        "        else:\n",
        "            opti = keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "    else:\n",
        "        print(\"Loading standard-format model\")\n",
        "        model = tf.keras.models.load_model(model_file)\n",
        "        print(\"Continuing training from existing model\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_model(save_path=None, on_tpu=False):\n",
        "    if is_tpu is True and on_tpu is True:\n",
        "        tpu_is_init=False\n",
        "        if tpu_is_init is False:\n",
        "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "            # tf.config.experimental_connect_to_cluster(cluster_resolver) # eager mode only! not TPU!\n",
        "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
        "            tpu_is_init=True\n",
        "        with tpu_strategy.scope():\n",
        "            print(\"Creating TPU-scope model\")\n",
        "            model = create_load_model(save_path=None)\n",
        "        if weights_file is not None and os.path.exists(weights_file):\n",
        "            print(\"Injecting saved weights into TPU model, loading...\")\n",
        "            temp_model = create_load_model(save_path=None)\n",
        "            temp_model.load_weights(weights_file)\n",
        "            print(\"Injecting...\")\n",
        "            model.set_weights(temp_model.get_weights())\n",
        "            print(\"Updated TPU weights from saved model\")\n",
        "        return model\n",
        "    else:\n",
        "        print(\"Creating standard-scope model\")\n",
        "        return create_load_model(save_path=save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZ9m9tbtnoO"
      },
      "source": [
        "def math_train(model, dataset, validation, batch_size=8192, epochs=5000, steps_per_epoch=2000):\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    interrupted = False\n",
        "    tensorboard_callback = callbacks.TensorBoard(\n",
        "        log_dir=\"./logs\",\n",
        "        histogram_freq=1,\n",
        "        write_images=1,\n",
        "        update_freq='batch')\n",
        "    try:\n",
        "        model.fit(dataset, validation_data=validation, epochs=epochs, steps_per_epoch=steps_per_epoch, # , batch_size=batch_size\n",
        "                  verbose=1, callbacks=[tensorboard_callback])  # validation_split=0.03 (not datasets!)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"---------INTERRUPT----------\")\n",
        "        print(\"\")\n",
        "        print(\"Training interrupted\")\n",
        "        interrupted = True\n",
        "    except Exception as e:\n",
        "        print(f\"Exception {e}\")\n",
        "    finally:\n",
        "        return interrupted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0SfAQMUvGYF"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380NmaHT7FqW"
      },
      "source": [
        "save_model = True\n",
        "model_file=None\n",
        "weight_file = None\n",
        "if is_colab:\n",
        "    mountpoint='/content/drive'\n",
        "    root_path='/content/drive/My Drive'\n",
        "    if not os.path.exists(root_path):\n",
        "        drive.mount(mountpoint)\n",
        "    if not os.path.exists(root_path):\n",
        "        print(f\"Something went wrong with Google Drive access. Cannot save model to {root_path}\")\n",
        "        save_model = False\n",
        "else:\n",
        "    root_path='.'\n",
        "\n",
        "if save_model:\n",
        "    if is_tpu is False:\n",
        "        if is_colab:\n",
        "            project_path=os.path.join(root_path,\"Colab Notebooks/ALU_Net\")\n",
        "        else:\n",
        "            project_path=root_path\n",
        "        model_file=os.path.join(project_path,'math_model')\n",
        "        print(f\"Model save-path: {model_file}\")\n",
        "    else:\n",
        "        project_path=os.path.join(root_path,\"Colab Notebooks/ALU_Net\")\n",
        "        weights_file=os.path.join(project_path,'math_model.h5')\n",
        "        print(f\"Weights save-path: {weights_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DsIyBlFvIr6"
      },
      "source": [
        "BATCH_SIZE = 20000\n",
        "SAMPLES = 2000000\n",
        "VALIDATION_SAMPLES = 80000\n",
        "EPOCHS_PER_MASTER_CYCLE = 100\n",
        "MASTER_CYCLES = 100\n",
        "STEPS_PER_EPOCH = SAMPLES // BATCH_SIZE\n",
        "REWEIGHT_SIZE = 1024\n",
        "\n",
        "VALID_OPS = None #['*']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoW9nkxcpM-T"
      },
      "source": [
        "# Initialize model(s)\n",
        "math_data = ALU_Dataset(pre_weight=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytnSNA0flOmJ"
      },
      "source": [
        "#dataset=math_data.create_dataset_from_generator()\n",
        "dataset = math_data.create_dataset(samples=SAMPLES, batch_size=BATCH_SIZE, short_math=False, valid_ops=VALID_OPS,title=\"Training\")\n",
        "val_dataset = math_data.create_dataset(samples=VALIDATION_SAMPLES, batch_size=BATCH_SIZE, short_math=False, valid_ops=VALID_OPS,title=\"Validation\")\n",
        "# val_dataset=math_data.create_dataset_from_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWev5OdEdpC"
      },
      "source": [
        "if is_tpu:\n",
        "    # Generate a second CPU model for testing:\n",
        "    test_model = get_model(save_path=None, on_tpu=False)\n",
        "math_model = get_model(save_path=model_file, on_tpu=is_tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn6F0aI_tsXZ"
      },
      "source": [
        "# Training\n",
        "for _ in range(0, MASTER_CYCLES):\n",
        "    #dataset = math_data.create_dataset(\n",
        "    #    samples=SAMPLES, batch_size=BATCH_SIZE, short_math=False, valid_ops=VALID_OPS)\n",
        "    #val_dataset = math_data.create_dataset(samples=VALIDATION_SAMPLES, batch_size=BATCH_SIZE, short_math=False, valid_ops=VALID_OPS)\n",
        "    interrupted = math_train(math_model, dataset, validation=val_dataset, epochs=EPOCHS_PER_MASTER_CYCLE, steps_per_epoch=STEPS_PER_EPOCH)\n",
        "    # interrupted = math_train(math_model, dataset, validation=val_dataset, epochs=EPOCHS_PER_MASTER_CYCLE) #, steps_per_epoch=STEPS_PER_EPOCH)\n",
        "    if is_tpu:\n",
        "        print(\"Injecting weights into test_model:\")\n",
        "        test_model.set_weights(math_model.get_weights())\n",
        "        if weights_file is not None:\n",
        "            print(f\"Saving test-model weights to {weights_file}\")\n",
        "            test_model.save_weights(weights_file)\n",
        "            print(\"Done\")\n",
        "        math_data.check_results(test_model, samples=REWEIGHT_SIZE, short_math=False, valid_ops=VALID_OPS, verbose=False)\n",
        "    else:\n",
        "        print(\"Saving math-model\")\n",
        "        math_model.save(model_file)\n",
        "        print(\"Done\")\n",
        "        math_data.check_results(math_model, samples=REWEIGHT_SIZE, short_math=False, valid_ops=VALID_OPS, verbose=False)\n",
        "    if interrupted:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcGr9h_p9_R"
      },
      "source": [
        "math_data.check_results(test_model, samples=100, short_math=False, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv8ZRa8GpThA"
      },
      "source": [
        "dx,dy,_,_,_=math_data.create_data_point(22,33,'+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZt0CbNdqpqW"
      },
      "source": [
        "math_data.decode_results(test_model.predict(np.array([dx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV52DL3gq0rI"
      },
      "source": [
        "def calc(inp):\n",
        "    args=inp.split(' ')\n",
        "    if len(args)!=3:\n",
        "        print(\"need three space separated tokens: <int> <operator> <int>, e.g. '3 + 4' or '4 XOR 5'\")\n",
        "        return False\n",
        "    if args[1] not in math_data.model_ops:\n",
        "        print(f\"{args[1]} is not a known operator.\")\n",
        "        return False\n",
        "    op1=int(args[0])\n",
        "    op2=int(args[2])\n",
        "    dx,dy,_,_,_=math_data.create_data_point(op1, op2, args[1])\n",
        "    ans=math_data.decode_results(test_model.predict(np.array([dx])))\n",
        "    print(f\"{op1} {args[1]} {op2} = {ans[0]}\")\n",
        "    return ans[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqeW9hlrEEE"
      },
      "source": [
        "calc(\"33 + 55\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jjSQodrH0s"
      },
      "source": [
        "calc(\"8812 = 8812\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss8eHRmXs4cE"
      },
      "source": [
        "999/27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoK-LUr-s9IO"
      },
      "source": [
        "calc(\"3 * 4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQNAv4Fs-_w"
      },
      "source": [
        "calc (\"1 AND 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIXKkzgNtBiG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}