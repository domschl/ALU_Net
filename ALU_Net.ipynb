{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XijwVpR4s0sQ"
   },
   "source": [
    "# Simulating an ALU (arithmetic logic unit) with a neural network\n",
    "\n",
    "The neural network is trained to perform the operations `+`, `-`, `*`, `/`, `%`, `AND`, `OR`, `XOR`, `>`, `<`, `=`, `!=` on two unsigned integers and return the result.\n",
    "\n",
    "## This notebook can run\n",
    "\n",
    "- on local jupyter instances with a local graphics card\n",
    "- on Mac M1 with local jupyter instance and [Apple's tensorflow-plugin](https://developer.apple.com/metal/tensorflow-plugin/)\n",
    "- on Google Colab instances with either GPU or TPU runtime. The colab version uses a Google Drive account to cache data and model state within a Google Drive directory `My Drive/Colab Notebooks/ALU_Net`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0ui7VokTJqc"
   },
   "source": [
    "## 1. Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ia2sNM2TTJqm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "use_keras_project_versions=False\n",
    "# Namespaces, namespaces\n",
    "if use_keras_project_versions is False:\n",
    "    # print(\"Importing Keras from tensorflow project (it won't work otherwise with TPU)\")\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers, callbacks, metrics, optimizers\n",
    "else:\n",
    "    # print(\"Importing Keras from keras project (which had recently declared independence [again]) -- as recommended\")\n",
    "    use_keras_project_versions=True\n",
    "    import keras\n",
    "    from keras import layers, regularizers, callbacks, metrics, optimizers\n",
    "\n",
    "try:\n",
    "    # Google Drive is used in Colab instances to save trained nets and tensorboard logs\n",
    "    from google.colab import drive\n",
    "    is_colab_init = True\n",
    "except:\n",
    "    is_colab_init = False\n",
    "    pass\n",
    "\n",
    "if is_colab_init is True:\n",
    "    # The following code loads the utility modules directly from github\n",
    "    # Into Google Colab (or other jupyter instances)\n",
    "    # WARNING: indeterministic caching by infrastructure: it might take some\n",
    "    # minutes until a change in github is actually accessible in colab (aggressive caching). Sometimes\n",
    "    # it's necessary to factory reset the runtime, and even that gets ignoredd from time to time.\n",
    "    def import_from_github(fn, repo_root_link, force_github_update=False):\n",
    "        if os.path.exists(fn) is False or force_github_update is True:\n",
    "            repo_link=repo_root_link+fn\n",
    "            print(f\"Loading {fn} module from github at {repo_link}...\")\n",
    "            if os.path.exists(fn) is True:\n",
    "                !rm -v {fn}\n",
    "            !wget -nv --show-progress {repo_link}\n",
    "    force_github_update = True  # Note: Even if set to True, you still need to restart the runtime to get an updated version.\n",
    "    repo_root_link = 'https://raw.githubusercontent.com/domschl/ALU_Net/main/'\n",
    "    import_from_github('ml_env_tools.py', repo_root_link, force_github_update)\n",
    "    import_from_github('ml_tuner.py', repo_root_link, force_github_update)\n",
    "    import_from_github('ALU_Dataset.py', repo_root_link, force_github_update)\n",
    "    import_from_github('keras_custom_layers.py', repo_root_link, force_github_update)\n",
    "\n",
    "from ml_env_tools import MLEnv\n",
    "from ml_tuner import MLTuner\n",
    "from ALU_Dataset import ALU_Dataset\n",
    "from keras_custom_layers import ResidualBlock, ResidualDense, ResidualDenseStack, ParallelResidualDenseStacks, SelfAttention, MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hZ_tOVlPdamx"
   },
   "outputs": [],
   "source": [
    "def model_res_mod(inputs, params):\n",
    "    # see: keras_custom_layers.py for layer definition:\n",
    "    x=inputs\n",
    "    print(f\"input-shape: {x.shape}\")\n",
    "    self_att=[]\n",
    "    for _ in range(0, params['self_attention_layers']):\n",
    "        self_att.append(MultiHeadSelfAttention(params['self_attention_heads'], units=params['self_attention_units']))\n",
    "    for i in range(0, params['self_attention_layers']):\n",
    "        x=self_att[i](x)+x\n",
    "    fl = layers.Flatten()\n",
    "    print(f\"x.shape bef. fl: {x.shape}\")\n",
    "    x = fl(x)\n",
    "    print(f\"x.shape after. fl: {x.shape}\")\n",
    "    scale = layers.Dense(params['units'], activation=None)\n",
    "    x=scale(x)\n",
    "    prds = ResidualDenseStack(params[\"units\"], params[\"layers\"], regularizer=params[\"regularizer\"])\n",
    "    x=prds(x)\n",
    "    rescale = layers.Dense(params['output_size'], activation=\"sigmoid\")\n",
    "    outputs = rescale(x)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xn3rAlvqgmWO"
   },
   "outputs": [],
   "source": [
    "def create_load_model(ml_env:MLEnv, model_variant, params, save_path=None, import_weights=True):\n",
    "    \"\"\" Create or load a model \"\"\"\n",
    "    if save_path is None or not os.path.exists(save_path) or import_weights is False: #or is_tpu is True:\n",
    "        print(\"Initializing new model...\")\n",
    "        # inputs = keras.Input(shape=(params['input_size'],))  # depends on encoding of op-code!\n",
    "        inputs = keras.Input(shape=params['input_size'])  # depends on encoding of op-code!\n",
    "        if model_variant not in model_variants:\n",
    "            print('Unkown model type')\n",
    "            return None\n",
    "        outputs = model_variants[model_variant](inputs, params)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths_\"+model_variant)\n",
    "        print(f\"Compiling new model of type {model_variant}\")\n",
    "        if use_keras_project_versions is False: \n",
    "            opti = keras.optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
    "        else:\n",
    "            opti = optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
    "        if ml_env.is_tpu:\n",
    "            # use steps_per_execution magic (or not)\n",
    "            # model.compile(loss=\"mean_squared_error\", optimizer=opti, steps_per_execution=50, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
    "            model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
    "        else:\n",
    "            model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
    "    else:\n",
    "        print(f\"Loading standard-format model of type {model_variant} from {model_path}\")\n",
    "        model = tf.keras.models.load_model(save_path)\n",
    "        print(\"Continuing training from existing model\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iqxHzunzgmWP"
   },
   "outputs": [],
   "source": [
    "def get_model(ml_env, model_variant, params, save_path=None, on_tpu=False, import_weights=False):\n",
    "    if on_tpu is True:\n",
    "        if ml_env.tpu_is_init is False:\n",
    "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=ml_env.tpu_address)\n",
    "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
    "            ml_env.tpu_is_init=True\n",
    "        with tpu_strategy.scope():\n",
    "            print(\"Creating TPU-scope model\")\n",
    "            model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
    "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
    "            print(\"Injecting saved weights into TPU model, loading...\")\n",
    "            temp_model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
    "            temp_model.load_weights(ml_env.weights_file)\n",
    "            print(\"Injecting...\")\n",
    "            model.set_weights(temp_model.get_weights())\n",
    "            print(\"Updated TPU weights from saved model\")\n",
    "        return model\n",
    "    else:\n",
    "        print(\"Creating standard-scope model\")\n",
    "        model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
    "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
    "            print(\"Injecting saved weights into model, loading...\")        \n",
    "            model.load_weights(ml_env.weights_file)\n",
    "            imported_weights_file = ml_env.weights_file+'-imported'\n",
    "            os.rename(ml_env.weights_file, imported_weights_file)\n",
    "            print(f\"Renamed weights file {ml_env.weights_file} to {imported_weights_file} to prevent further imports!\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y4HDT65wgmWP"
   },
   "outputs": [],
   "source": [
    "def math_train(mlenv:MLEnv, model, dataset, validation, batch_size=8192, epochs=5000, steps_per_epoch=2000, log_path=\"./logs\"):\n",
    "    \"\"\" Training loop \"\"\"\n",
    "    interrupted = 2\n",
    "    hist = None\n",
    "    tensorboard_callback = callbacks.TensorBoard(\n",
    "        log_dir=log_path\n",
    "        # histogram_freq=1\n",
    "        # update_freq='batch'\n",
    "        )\n",
    "    if mlenv.is_tpu is False: # TPUs update Tensorboard too asynchronously, data is corrupted by updates during mirroring.\n",
    "        lambda_callback = tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end = ml_env.epoch_time_func\n",
    "        )\n",
    "    try:\n",
    "        if ml_env.is_tpu:\n",
    "            if use_validation_with_tpu is True:\n",
    "                hist = model.fit(dataset, validation_data=validation, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1, callbacks=[tensorboard_callback])\n",
    "            else:\n",
    "                hist = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[tensorboard_callback])\n",
    "            interrupted=0\n",
    "        else:\n",
    "            hist = model.fit(dataset, validation_data=validation, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[tensorboard_callback, lambda_callback])\n",
    "            interrupted=0\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"---------INTERRUPT----------\")\n",
    "        print(\"\")\n",
    "        print(\"Training interrupted\")\n",
    "        interrupted = 1 # user stopped runtime\n",
    "    except Exception as e:\n",
    "        interruped = 2  # Bad: something crashed.\n",
    "        print(f\"INTERNAL ERROR\")\n",
    "        print(f\"Exception {e}\")\n",
    "    finally:\n",
    "        return interrupted, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z9_Ey8nDgmWP"
   },
   "outputs": [],
   "source": [
    "def instantiate_models(ml_env:MLEnv, model_variant, params, save_path=None, import_weights=True):\n",
    "    if ml_env.is_tpu:\n",
    "        # Generate a second CPU model for testing:\n",
    "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=True, import_weights=import_weights)\n",
    "        test_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
    "    else:\n",
    "        test_model = None\n",
    "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
    "    return math_model, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hL4L7FZYgmWP"
   },
   "outputs": [],
   "source": [
    "def do_training(mlenv:MLEnv, math_model, training_dataset, validation_dataset, math_data, epochs_per_cycle, model_path=None, \n",
    "                weights_file=None, test_model=None, cycles=100, steps_per_epoch=1000, reweight_size=1000, valid_ops=None, regenerate_data_after_cycles=0, data_func=None,\n",
    "                log_path='./logs'):\n",
    "    # Training\n",
    "    for mep in range(0, cycles):\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"------ Meta-Epoch {mep+1}/{cycles} ------\")\n",
    "        print()\n",
    "        if regenerate_data_after_cycles!=0 and data_func is not None:\n",
    "            if mep>0 and (mep+1)%regenerate_data_after_cycles==0:\n",
    "                training_dataset, validation_dataset = data_func()\n",
    "        if mep==0 and ml_env.is_tpu is True:\n",
    "            print(\"There will be some warnings by Tensorflow, documenting some state of internal decoherence, currently they can be ignored.\")\n",
    "        interrupted, hist = math_train(ml_env, math_model, training_dataset, validation=validation_dataset, epochs=epochs_per_cycle, steps_per_epoch=steps_per_epoch, log_path=log_path)\n",
    "        if interrupted <2:\n",
    "            if ml_env.is_tpu:\n",
    "                mlenv.gdrive_log_mirror()  # TPUs can only savely mirror Tensorboard data once training is finished for an meta-epoch.\n",
    "                if test_model is None:\n",
    "                    print(\"Fatal: tpu-mode needs test_model on CPU\")\n",
    "                    return False\n",
    "                print(\"Injecting weights into test_model:\")\n",
    "                test_model.set_weights(math_model.get_weights())\n",
    "                if weights_file is not None:\n",
    "                    print(f\"Saving test-model weights to {weights_file}\")\n",
    "                    test_model.save_weights(weights_file)\n",
    "                    print(\"Done\")\n",
    "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
    "                math_data.check_results(test_model, samples=reweight_size, vector=vector, valid_ops=valid_ops, verbose=False)\n",
    "            else:\n",
    "                if model_path is not None:\n",
    "                    print(\"Saving math-model\")\n",
    "                    math_model.save(model_path)\n",
    "                    print(\"Done\")\n",
    "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
    "                math_data.check_results(math_model, samples=reweight_size, vector=vector, valid_ops=valid_ops, verbose=False)\n",
    "        if interrupted>0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4b07h1AVAmtI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] {}\n",
      "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] {'device_name': 'METAL'}\n",
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "ml_env=MLEnv()\n",
    "bit_count = 15\n",
    "math_data=ALU_Dataset(ml_env, bit_count = bit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9yjwuceDgmWQ"
   },
   "outputs": [],
   "source": [
    "model_variants = {\"res_mod\": model_res_mod,\n",
    "                  }\n",
    "\n",
    "model_variant = 'res_mod'  # see: model_variants definition.\n",
    "epochs_per_cycle = 100\n",
    "cycles = 100  # perform 100 (meta-)cycles, each cycle trains with epochs_per_cycle epochs.\n",
    "regenerate_data_after_cycles = 0  # if !=0, the training data will be created anew after each number of \n",
    "                                  # regenerace_data_after_cycles cycles. Disadvantage: when training TPU, \n",
    "                                  # Google might use the time it takes to regenerate to training data to \n",
    "                                  # terminate your session :-/\n",
    "low_resource = True\n",
    "\n",
    "if low_resource is True:\n",
    "    samples = 100000  # Number training data examples. \n",
    "                    # WARNING: TPU simply crashes, if 2GB limit for entire set is reached.\n",
    "                    # Possible solutions: https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#running_the_tfdata_service,\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/data/experimental/service , https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_input_pipeline.py#L33\n",
    "    validation_samples=10000\n",
    "else:\n",
    "    samples = 4000000  # Number training data examples. \n",
    "                    # WARNING: TPU simply crashes, if 2GB limit for entire set is reached.\n",
    "                    # Possible solutions: https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#running_the_tfdata_service,\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/data/experimental/service , https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_input_pipeline.py#L33\n",
    "    validation_samples=100000\n",
    "    \n",
    "if low_resource is True:\n",
    "    batch_size = 2000\n",
    "else:\n",
    "    batch_size = 20000\n",
    "import_weights=True\n",
    "if import_weights is False:\n",
    "    print(\"WARNING: import weights is set to False!\")\n",
    "valid_ops = None  # Default: None (all ops), or list of ops, e.g. ['*', '/'] trains only multiplication and division.\n",
    "# valid_ops = ['*','/','+','-']\n",
    "# valid_ops = ['*']\n",
    "steps_per_epoch = samples // batch_size  # TPU stuff\n",
    "validation_steps= validation_samples // batch_size  # again TPU only\n",
    "use_validation_with_tpu = False  # Is somehow really, really slow\n",
    "\n",
    "params_res_mod={\n",
    "    \"self_attention_layers\": 4,\n",
    "    \"self_attention_heads\": 4,\n",
    "    \"self_attention_units\": 64,\n",
    "    \"units\": 64,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"layers\": 1,\n",
    "    \"regularizer\": 1e-9\n",
    "    }\n",
    "\n",
    "params=params_res_mod\n",
    "vector = True\n",
    "\n",
    "if vector is True:\n",
    "    params['input_size'] = [3, math_data.embedding_size]\n",
    "else:\n",
    "    params['input_size'] = math_data.input_size\n",
    "params['output_size'] = math_data.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tnBLx9GwgmWQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path: .\n",
      "Model save-path: ./math_model_res_mod\n",
      "Data cache path ./data\n"
     ]
    }
   ],
   "source": [
    "root_path, project_path, model_path, weights_file, cache_path, log_path = ml_env.init_paths(\"ALU_Net\", \"math_model\", model_variant=model_variant, log_to_gdrive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9iliMhQoN2UU"
   },
   "outputs": [],
   "source": [
    "apply_model_tuner = False   # Use GPU (not TPU!) for model_tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r7ZPfXQdPe-d"
   },
   "outputs": [],
   "source": [
    "if apply_model_tuner is True:\n",
    "    as_train, as_val = math_data.get_datasets(samples=500000, validation_samples=50000, vector=vector, cache_path=cache_path)\n",
    "\n",
    "    def tuner_eval(ml_env:MLEnv, model_variant, params, batch_size, epochs):\n",
    "        math_model, _ = instantiate_models(ml_env, model_variant, params, save_path=None, import_weights=False)\n",
    "        interrupted, hist = math_train(ml_env, math_model, as_train, as_val, batch_size=batch_size, epochs=epochs)\n",
    "        print(params, end=\" [ \")\n",
    "        res = math_data.check_results(math_model, samples=100, valid_ops=valid_ops, verbose=False)\n",
    "        ev = 1/hist.history['val_loss'][-1]+hist.history['val_accuracy'][-1]*20\n",
    "        if res>0:\n",
    "            print(\"Success-rate: {res}\")\n",
    "            ev += res*5000\n",
    "        return ev\n",
    "\n",
    "    tuner_eval_func = lambda params : tuner_eval(ml_env, model_variant, params, batch_size=batch_size, epochs=20)\n",
    "    ml_tuner = MLTuner(ml_env, model_variant)\n",
    "\n",
    "    param_space_minimal_prm={\n",
    "    \"dense_layers\": [4,8,12],\n",
    "    \"dense_neurons\":[256,512,768], \n",
    "    \"learning_rate\": [0.001,0.002],\n",
    "    \"regu1\": [1e-8,1e-7]\n",
    "    }\n",
    "\n",
    "    best_params = ml_tuner.tune(param_space_minimal_prm, tuner_eval_func)\n",
    "    params = best_params\n",
    "    import_weights=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GCOAdbtPfQki"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_attention_layers': 4,\n",
       " 'self_attention_heads': 4,\n",
       " 'self_attention_units': 64,\n",
       " 'units': 64,\n",
       " 'learning_rate': 0.002,\n",
       " 'layers': 1,\n",
       " 'regularizer': 1e-09,\n",
       " 'input_size': [3, 16],\n",
       " 'output_size': 32}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2wN1G_qPgmWQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data train loaded from cache\n",
      "Metal device set to: Apple M1\n",
      "Data validation loaded from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 07:58:15.091083: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-07 07:58:15.091188: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "create_train_val_data = lambda regen : math_data.get_datasets(pre_weight=False, samples=samples, validation_samples=validation_samples, batch_size=batch_size, \n",
    "                                     vector=vector, valid_ops=valid_ops, cache_path=cache_path, use_cache=True, regenerate_cached_data=regen)\n",
    "create_train_val_data_regen = lambda : create_train_val_data(True)\n",
    "train, val = create_train_val_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mbZ_R7anJRda"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "# import tensorflow_datasets as tdfs\n",
    "# sa=SelfAttention()\n",
    "# nval=tdfs.as_numpy(val)\n",
    "# for n in nval:\n",
    "#     print(n[0].shape)   \n",
    "#     print(sa(n[0]).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iSL6lysogmWQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating standard-scope model\n",
      "Initializing new model...\n",
      "input-shape: (None, 3, 16)\n",
      "x.shape bef. fl: (None, 3, 16)\n",
      "x.shape after. fl: (None, 48)\n",
      "Compiling new model of type res_mod\n",
      "Model: \"maths_res_mod\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3, 16)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_self_attention (Mult (None, 3, 16)        3172        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 3, 16)        0           multi_head_self_attention[0][0]  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_self_attention_1 (Mu (None, 3, 16)        3172        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 3, 16)        0           multi_head_self_attention_1[0][0]\n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_self_attention_2 (Mu (None, 3, 16)        3172        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 3, 16)        0           multi_head_self_attention_2[0][0]\n",
      "                                                                 tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_self_attention_3 (Mu (None, 3, 16)        3172        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 3, 16)        0           multi_head_self_attention_3[0][0]\n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 48)           0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           3136        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "residual_dense_stack (ResidualD (None, 64)           4416        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        residual_dense_stack[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 22,320\n",
      "Trainable params: 22,192\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "math_model, test_model = instantiate_models(ml_env, model_variant, params, save_path=model_path, import_weights=import_weights)\n",
    "# math_model, test_model = instantiate_models(ml_env, model_variant, params, save_path=None, import_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Epfwj4czgmWQ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # use the python variable log_path:\n",
    "    get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntW3khTWgmWQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------ Meta-Epoch 1/100 ------\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 07:58:15.530261: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-07 07:58:15.530273: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-07 07:58:15.530447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-07 07:58:16.785490: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-07 07:58:16.795829: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-12-07 07:58:16.796207: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/50 [..............................] - ETA: 1:48 - loss: 0.3196 - mean_squared_error: 0.3196 - accuracy: 0.0390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 07:58:17.832158: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-07 07:58:17.832170: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/50 [>.............................] - ETA: 14s - loss: 0.3118 - mean_squared_error: 0.3118 - accuracy: 0.0383 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 07:58:18.081128: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-07 07:58:18.104279: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-07 07:58:18.123342: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2021_12_07_07_58_18\n",
      "\n",
      "2021-12-07 07:58:18.139985: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.trace.json.gz\n",
      "2021-12-07 07:58:18.145285: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2021_12_07_07_58_18\n",
      "\n",
      "2021-12-07 07:58:18.145489: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.memory_profile.json.gz\n",
      "2021-12-07 07:58:18.146457: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/train/plugins/profile/2021_12_07_07_58_18\n",
      "Dumped tool data for xplane.pb to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2021_12_07_07_58_18/m1air.fritz.box.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 0.1350 - mean_squared_error: 0.1350 - accuracy: 0.1059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 07:58:30.312034: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 15s 266ms/step - loss: 0.1350 - mean_squared_error: 0.1350 - accuracy: 0.1059 - val_loss: 0.0958 - val_mean_squared_error: 0.0958 - val_accuracy: 0.1962\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 0.0746 - mean_squared_error: 0.0746 - accuracy: 0.1709 - val_loss: 0.0761 - val_mean_squared_error: 0.0761 - val_accuracy: 0.0813\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 0.0676 - mean_squared_error: 0.0676 - accuracy: 0.2031 - val_loss: 0.0780 - val_mean_squared_error: 0.0780 - val_accuracy: 0.1098\n",
      "Epoch 4/100\n",
      "38/50 [=====================>........] - ETA: 2s - loss: 0.0646 - mean_squared_error: 0.0646 - accuracy: 0.2243"
     ]
    }
   ],
   "source": [
    "do_training(ml_env, math_model, train, val, math_data, epochs_per_cycle, model_path=model_path, \n",
    "            weights_file=weights_file, test_model=test_model, cycles=cycles, steps_per_epoch=steps_per_epoch, valid_ops=valid_ops, \n",
    "            regenerate_data_after_cycles=regenerate_data_after_cycles, data_func=create_train_val_data_regen, log_path=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO6dRCu6TJqm"
   },
   "source": [
    "# Testing and applying the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hcGr9h_p9_R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ml_env.is_tpu is False:\n",
    "    test_model = math_model\n",
    "math_data.check_results(test_model, samples=100, valid_ops=valid_ops, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv8ZRa8GpThA"
   },
   "outputs": [],
   "source": [
    "dx,dy,_,_,_=math_data.create_data_point(22,33,'*'); print(22*33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZt0CbNdqpqW"
   },
   "outputs": [],
   "source": [
    "r=test_model.predict(np.array([dx]))\n",
    "print(r)\n",
    "math_data.decode_results(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV52DL3gq0rI"
   },
   "outputs": [],
   "source": [
    "def calc(inp):\n",
    "    args=inp.split(' ')\n",
    "    if len(args)!=3:\n",
    "        print(\"need three space separated tokens: <int> <operator> <int>, e.g. '3 + 4' or '4 XOR 5'\")\n",
    "        return False\n",
    "    if args[1] not in math_data.model_ops:\n",
    "        print(f\"{args[1]} is not a known operator.\")\n",
    "        return False\n",
    "    op1=int(args[0])\n",
    "    op2=int(args[2])\n",
    "    dx,dy,_,_,_=math_data.create_data_point(op1, op2, args[1])\n",
    "    ans=math_data.decode_results(test_model.predict(np.array([dx])))\n",
    "    print(f\"{op1} {args[1]} {op2} = {ans[0]}\")\n",
    "    op=f\"{op1} {args[1]} {op2}\"\n",
    "    op=op.replace('AND', '&').replace('XOR','^').replace('=','==').replace('OR','|')\n",
    "    an2=eval(op)\n",
    "    if ans[0]!=an2:\n",
    "        print(\"Error\")\n",
    "        print(bin(ans[0]))\n",
    "        print(bin(an2))\n",
    "    return ans[0],an2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeqeW9hlrEEE"
   },
   "outputs": [],
   "source": [
    "calc(\"22 * 33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0jjSQodrH0s"
   },
   "outputs": [],
   "source": [
    "calc(\"1 = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoK-LUr-s9IO"
   },
   "outputs": [],
   "source": [
    "calc(\"3 * 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frQNAv4Fs-_w"
   },
   "outputs": [],
   "source": [
    "calc (\"1 AND 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL8L83LllnoZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "ALU_Net.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
