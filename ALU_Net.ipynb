{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ALU_Net.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XijwVpR4s0sQ"
      },
      "source": [
        "# A neural net that tries to become an ALU (arithmetic logic unit)\n",
        "\n",
        "This notebook can run\n",
        "\n",
        "- on local jupyter instances with a local graphics card\n",
        "- on Mac M1 with local jupyter instance and [Apple's tensorflow-plugin](https://developer.apple.com/metal/tensorflow-plugin/)\n",
        "- on Google Colab instances with either GPU or TPU runtime. The colab version uses a Google Drive account to cache data and model state within a Google Drive directory `My Drive/Colab Notebooks/ALU_Net`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ui7VokTJqc"
      },
      "source": [
        "## 1. Configuration and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv54Aqp9s_gs"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoLypnLqxL4p"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try: # Colab instance?\n",
        "    from google.colab import drive\n",
        "    is_colab = True\n",
        "except: # Not? ignore.\n",
        "    is_colab = False\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfh0Gsn4ISLY"
      },
      "source": [
        "# Hardware check:\n",
        "\n",
        "is_tpu = False\n",
        "is_gpu = False\n",
        "tpu_is_init = False\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hw_list=tf.config.experimental.list_physical_devices(hw)\n",
        "    if len(hw_list)>0:\n",
        "        if hw=='TPU':\n",
        "            is_tpu=True\n",
        "        if hw=='GPU':\n",
        "            is_gpu=True\n",
        "        print(f\"{hw}: {hw_list} {tf.config.experimental.get_device_details(hw_list[0])}\") \n",
        "\n",
        "if is_colab:\n",
        "    if not is_tpu:\n",
        "        try:\n",
        "            TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "            tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "            is_tpu = True\n",
        "            print(f\"TPU available at {TPU_ADDRESS}\")\n",
        "        except:\n",
        "            print(\"No TPU available\")\n",
        "    else:\n",
        "        print(f\"TPU available, already connected to {TPU_ADDRESS}\")\n",
        "\n",
        "if not is_tpu:\n",
        "    if not is_gpu:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "    else:\n",
        "        print(\"GPU available\")\n",
        "else:\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    print(\"TPU: eager execution disabled!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrQx2YThMYDC"
      },
      "source": [
        "use_keras_project=False\n",
        "\n",
        "# Namespaces, namespaces\n",
        "if use_keras_project is False:\n",
        "    print(\"Importing Keras from tensorflow project (it won't work otherwise with TPU)\")\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "else:\n",
        "    print(\"Importing Keras from keras project (which had recently declared independence [again]) -- as recommended\")\n",
        "    import keras\n",
        "    from keras import layers, regularizers, callbacks, metrics, optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BtiGmrMTJqe"
      },
      "source": [
        "def init_paths(model_type='large', log_to_gdrive=False):\n",
        "    save_model = True\n",
        "    model_file=None\n",
        "    cache_stub=None\n",
        "    weights_file = None\n",
        "    project_path = None\n",
        "    log_path = \"./logs\"\n",
        "    if is_colab:\n",
        "        mountpoint='/content/drive'\n",
        "        root_path='/content/drive/My Drive'\n",
        "        print(\"You will now be asked to authenticate Google Drive access in order to store training data (cache) and model state.\")\n",
        "        print(\"Changes will only happen within Google Drive directory `My Drive/Colab Notebooks/ALU_Net`.\")\n",
        "        if not os.path.exists(root_path):\n",
        "            drive.mount(mountpoint)\n",
        "        if not os.path.exists(root_path):\n",
        "            print(f\"Something went wrong with Google Drive access. Cannot save model to {root_path}\")\n",
        "            save_model = False\n",
        "    else:\n",
        "        root_path='.'\n",
        "\n",
        "    if save_model:\n",
        "        if is_colab:\n",
        "            project_path=os.path.join(root_path,\"Colab Notebooks/ALU_Net\")\n",
        "            if log_to_gdrive is True:\n",
        "                log_path = os.path.join(root_path,\"Colab Notebooks/ALU_Net/logs\")\n",
        "        else:\n",
        "            project_path=root_path\n",
        "        model_file=os.path.join(project_path,f'math_model_{model_type}')\n",
        "        cache_stub=os.path.join(project_path,'data_cache')\n",
        "        weights_file=os.path.join(project_path,f'math_model_{model_type}.h5')\n",
        "        if is_tpu is False:\n",
        "            print(f\"Model save-path: {model_file}\")\n",
        "        else:\n",
        "            print(f\"Weights save-path: {weights_file}\")\n",
        "        print(f'Data cache file-stub {cache_stub}')\n",
        "    return model_file, weights_file, cache_stub, log_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WoVJ0wxTJqf"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0TucwitE4T",
        "tags": []
      },
      "source": [
        "class ALU_Dataset():\n",
        "    \"\"\" Generate training data for all ALU operations \"\"\"\n",
        "    # The ALU takes two integers and applies one of the supported\n",
        "    # model_ops. Eg op1=123, op2=100, op='-' -> result 23\n",
        "    # The net is supposed to learn to 'calculate' the results for\n",
        "    # arbitrary op1, op2 (positive integers, 0..32767) and \n",
        "    # the twelve supported ops \n",
        "\n",
        "    def __init__(self, pre_weight=False):\n",
        "        self.model_ops = [\"+\", \"-\", \"*\", \"/\", \"%\",\n",
        "                          \"AND\", \"OR\", \"XOR\", \">\", \"<\", \"=\", \"!=\"]\n",
        "        self.model_is_boolean = [False, False, False, False, False,\n",
        "                                 False, False, False, True, True, True, True]\n",
        "        # Probabilites for creating a sample for each of the ops, (Will be\n",
        "        # reweighted on checks to generate for samples for 'difficult' ops):\n",
        "        self.model_dis = [10, 10, 10, 10, 10, 10,   10,  10,   10, 10, 10, 10]\n",
        "        model_dis_w = [19, 12, 110, 15, 36, 10, 10, 10, 10, 10, 10, 10]\n",
        "        self.model_funcs = [self.add_smpl, self.diff_smpl, self.mult_smpl,\n",
        "                            self.div_smpl, self.mod_smpl, self.and_smpl,\n",
        "                            self.bor_smpl, self.xor_smpl, self.greater_smpl,\n",
        "                            self.lesser_smpl, self.eq_smpl, self.neq_smpl]\n",
        "        self.bit_count = 15\n",
        "        self.all_bits_one = 0x7fffffff\n",
        "        self.true_vect = self.all_bits_one\n",
        "        self.false_vect = 0\n",
        "        if pre_weight is True:\n",
        "            self.model_dis=model_dis_w\n",
        "\n",
        "    @staticmethod\n",
        "    def int_to_binary_vect(num_int, num_bits=8):\n",
        "        \"\"\" get a binary encoded vector of n of bit-lenght nm \"\"\"\n",
        "        num_vect = np.zeros(num_bits, dtype=np.float32)\n",
        "        for i in range(0, num_bits):\n",
        "            if num_int & (2**i) != 0:\n",
        "                num_vect[i] = 1.0\n",
        "        return num_vect\n",
        "\n",
        "    @staticmethod\n",
        "    def get_random_bits(bits):\n",
        "        \"\"\" get bits random int 0...2**bits-1 \"\"\"\n",
        "        return random.randint(0, 2**bits-1)\n",
        "\n",
        "    def op_string_to_index(self, op_string):\n",
        "        \"\"\" transform op_string (e.g. '+' -> 0) into corresponding index \"\"\"\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            if self.model_ops[i] == op_string:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def get_data_point(self, equal_distrib=False, short_math=False, valid_ops=None):\n",
        "        \"\"\" Get a random example for on ALU operation for training \"\"\"\n",
        "        result = -1\n",
        "        op1 = self.get_random_bits(self.bit_count)\n",
        "        op2 = self.get_random_bits(self.bit_count)\n",
        "        if valid_ops is not None and len(valid_ops)==0:\n",
        "            valid_ops=None\n",
        "        if valid_ops is not None:\n",
        "            if equal_distrib is False:\n",
        "                print(\"Op restriction via valid_ops forces equal_distrib=True\")\n",
        "                equal_distrib=True\n",
        "            for op in valid_ops:\n",
        "                if op not in self.model_ops:\n",
        "                    print(f'Cannot restrict valid_ops to {op}, unknown operation, ignoring all valid_ops')\n",
        "                    valid_ops=None\n",
        "                    break\n",
        "\n",
        "        if equal_distrib or valid_ops is not None:\n",
        "            if valid_ops is None:   \n",
        "                op_index = random.randint(0, len(self.model_ops)-1)\n",
        "            else:\n",
        "                if len(valid_ops)==1:\n",
        "                    op_index=0\n",
        "                else:\n",
        "                    op_index = random.randint(0, len(valid_ops)-1)\n",
        "                op_index=self.model_ops.index(valid_ops[op_index])\n",
        "        else: # make 'difficult' ops more present in training samples:\n",
        "            rx = 0\n",
        "            for md in self.model_dis:\n",
        "                rx += md\n",
        "            rrx = random.randint(0, rx)\n",
        "            rx = 0\n",
        "            op_index = 0\n",
        "            for op_index in range(0, len(self.model_ops)):\n",
        "                rx += self.model_dis[op_index]\n",
        "                if rx > rrx:\n",
        "                    break\n",
        "        return self.encode_op(op1, op2, op_index, short_math)\n",
        "\n",
        "    def generator(self, samples=20000, equal_distrib=False, short_math=False, valid_ops=None):\n",
        "        while True:\n",
        "            x, Y = self.create_training_data(samples=samples, short_math=short_math, valid_ops=valid_ops, verbose=False, title=None)\n",
        "            #x, Y, _, _, _ = self.get_data_point(equal_distrib=equal_distrib, short_math=short_math, valid_ops=valid_ops)\n",
        "            yield x, Y\n",
        "\n",
        "    def encode_op(self, op1, op2, op_index, short_math=False):\n",
        "        \"\"\" turn two ints and operation into training data \"\"\"\n",
        "        op1, op2, result = self.model_funcs[op_index](op1, op2, short_math)\n",
        "        if self.model_is_boolean[op_index] is True:\n",
        "            if result==self.false_vect:\n",
        "                str_result=\"False\"\n",
        "            elif result==self.true_vect:\n",
        "                str_result=\"True\"\n",
        "            else:\n",
        "                str_result=\"undefined\"\n",
        "        else:\n",
        "            str_result=result\n",
        "        sym = f\"{op1} {self.model_ops[op_index]} {op2} = {str_result}\"\n",
        "        inp = np.concatenate(\n",
        "            [self.int_to_binary_vect(op1, num_bits=16),\n",
        "             self.int_to_binary_vect(op_index, num_bits=4),\n",
        "             self.int_to_binary_vect(op2, num_bits=16)])\n",
        "        oup = self.int_to_binary_vect(result, num_bits=32)\n",
        "        return inp, oup, result, op_index, sym\n",
        "\n",
        "    @staticmethod\n",
        "    def add_smpl(op1, op2, _):\n",
        "        \"\"\" addition training example \"\"\"\n",
        "        result = op1+op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def diff_smpl(op1, op2, _):\n",
        "        \"\"\" subtraction training example \"\"\"\n",
        "        if op2 > op1:\n",
        "            op2, op1 = op1, op2\n",
        "        result = op1-op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def mult_smpl(op1, op2, short_math=False):\n",
        "        \"\"\" multiplication training example \"\"\"\n",
        "        if short_math:\n",
        "            op1 = op1 % 1000\n",
        "            op2 = op2 % 1000\n",
        "        result = op1*op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def div_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer division training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1//op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def mod_smpl(self, op1, op2, _):\n",
        "        \"\"\" modulo (remainder) training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1 % op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def and_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise AND training example \"\"\"\n",
        "        result = op1 & op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def bor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise OR training example \"\"\"\n",
        "        result = op1 | op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    @staticmethod\n",
        "    def xor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise XOR training example \"\"\"\n",
        "        result = op1 ^ op2\n",
        "        return op1, op2, result\n",
        "\n",
        "    def greater_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation > training example \"\"\"\n",
        "        if op1 > op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def lesser_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation < training example \"\"\"\n",
        "        if op1 < op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def eq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation == training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 == op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def neq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation != training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 != op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return op1, op2, result\n",
        "\n",
        "    def create_data_point(self, op1, op2, op_string):\n",
        "        \"\"\" create training data from given ints op1, op2 and op_string \"\"\"\n",
        "        op_index = self.op_string_to_index(op_string)\n",
        "        if op_index == -1:\n",
        "            print(f\"Invalid operation {op_string}\")\n",
        "            return np.array([]), np.array([]), -1, -1, None\n",
        "        return self.encode_op(op1, op2, op_index)\n",
        "\n",
        "    def create_training_data(self, samples=10000, short_math=False, valid_ops=None, verbose=True, title=None):\n",
        "        \"\"\" create a number of training samples \"\"\"\n",
        "        x, y, _, _, _ = self.get_data_point()\n",
        "        dpx = np.zeros((samples, len(x)), dtype=np.float32)\n",
        "        dpy = np.zeros((samples, len(y)), dtype=np.float32)\n",
        "        if verbose is True:\n",
        "            if title is None:\n",
        "                print(f\"Creating {samples} data points (. = 1000 progress)\")\n",
        "            else:\n",
        "                print(f\"{title}: Creating {samples} data points (. = 1000 progress)\")\n",
        "\n",
        "        for i in range(0, samples):\n",
        "            if verbose is True:\n",
        "                if i%100000 == 0:\n",
        "                    print(f\"{i:>10} \", end=\"\")\n",
        "            if (i+1) % 1000 == 0:\n",
        "                if verbose is True:\n",
        "                    print(\".\", end=\"\")\n",
        "                    sys.stdout.flush()\n",
        "                    if (i+1) % 100000 == 0:\n",
        "                        print()\n",
        "            if valid_ops is None:\n",
        "                x, y, _, _, _ = self.get_data_point(\n",
        "                    equal_distrib=False, short_math=short_math)\n",
        "            else:\n",
        "                x, y, _, _, _ = self.get_data_point(\n",
        "                    equal_distrib=True, short_math=short_math, valid_ops=valid_ops)\n",
        "            dpx[i, :] = x\n",
        "            dpy[i, :] = y\n",
        "        if verbose is True:\n",
        "            print()\n",
        "        return dpx, dpy\n",
        "\n",
        "    def create_dataset(self, samples=10000, batch_size=2000, short_math=False, valid_ops=None, name=None, cache_file=None, use_cache=True, regenerate_cached_data=False):\n",
        "        is_loaded=False\n",
        "        if use_cache is True:\n",
        "            if valid_ops is not None:\n",
        "                infix='_'\n",
        "                for vo in valid_ops:\n",
        "                    if vo=='*': # Prevent poison-filenames\n",
        "                        vo=\"MULT\"\n",
        "                    if vo=='/':\n",
        "                        vo=\"DIV\"\n",
        "                    if vo=='%':\n",
        "                        vo='MOD'\n",
        "                    if vo=='<':\n",
        "                        vo='LT'\n",
        "                    if vo=='>':\n",
        "                        vo='GT'\n",
        "                    if vo=='=':\n",
        "                        vo='EQ'\n",
        "                    if vo=='!=':\n",
        "                        vo='NE'\n",
        "                    infix+=vo\n",
        "            else:\n",
        "                infix=\"\"\n",
        "            cache_file_x=cache_file+infix+'_x.npy'\n",
        "            cache_file_Y=cache_file+infix+\"_Y.npy\"\n",
        "        if use_cache is True  and regenerate_cached_data is False and os.path.exists(cache_file_x) and os.path.exists(cache_file_Y):\n",
        "            try:\n",
        "                x = np.load(cache_file_x, allow_pickle=True)\n",
        "                Y = np.load(cache_file_Y, allow_pickle=True)\n",
        "                if len(x)==samples:\n",
        "                    is_loaded=True\n",
        "                    print(f\"Data {name} loaded from cache\")\n",
        "                else:\n",
        "                    print(f\"Sample count has changed from {len(x)} to {samples}, regenerating {name} data...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Something went wrong when loading {cache_file_x}, {cache_file_Y}: {e}\")\n",
        "        if is_loaded is False:\n",
        "            x, Y = self.create_training_data(samples=samples, short_math=short_math, valid_ops=valid_ops, title=name)\n",
        "            if use_cache is True:\n",
        "                print(f\"Writing data-cache {cache_file_x}, {cache_file_Y}...\")\n",
        "                np.save(cache_file_x, x, allow_pickle=True)\n",
        "                np.save(cache_file_Y, Y, allow_pickle=True)\n",
        "        shuffle_buffer=10000\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((x, Y)).cache()\n",
        "        dataset=dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
        "        if is_tpu is True:\n",
        "            dataset=dataset.repeat() # Mandatory for Keras TPU for now\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "        dataset=dataset.prefetch(-1) # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "        return dataset\n",
        "\n",
        "    def create_dataset_from_generator(self, short_math=False, valid_ops=None):\n",
        "        dataset=tf.data.Dataset.from_generator(\n",
        "            self.generator,\n",
        "            output_signature=(\n",
        "                    tf.TensorSpec(shape=(None,36), dtype=np.float32),\n",
        "                    tf.TensorSpec(shape=(None,32), dtype=np.float32))\n",
        "            )\n",
        "        return dataset\n",
        "        \n",
        "    @staticmethod\n",
        "    def decode_results(result_int_vects):\n",
        "        \"\"\" take an array of 32-float results from neural net and convert to ints \"\"\"\n",
        "        result_vect_ints = []\n",
        "        for vect in result_int_vects:\n",
        "            if (len(vect) != 32):\n",
        "                print(f\"Ignoring unexpected vector of length {len(vect)}\")\n",
        "            else:\n",
        "                int_result = 0\n",
        "                for i in range(0, 32):\n",
        "                    if vect[i] > 0.5:\n",
        "                        int_result += 2**i\n",
        "                result_vect_ints.append(int_result)\n",
        "        return result_vect_ints\n",
        "\n",
        "    def check_results(self, model, samples=1000, short_math=False, valid_ops=None, verbose=False):\n",
        "        \"\"\" Run a number of tests on trained model \"\"\"\n",
        "        ok = 0\n",
        "        err = 0\n",
        "        operr = [0]*len(self.model_ops)\n",
        "        opok = [0]*len(self.model_ops)\n",
        "        for _ in range(0, samples):\n",
        "            x, _, z, op, s = self.get_data_point(\n",
        "                equal_distrib=True, valid_ops=valid_ops, short_math=short_math)\n",
        "            res = self.decode_results(model.predict(np.array([x])))\n",
        "            if res[0] == z:\n",
        "                ok += 1\n",
        "                opok[op] += 1\n",
        "                r = \"OK\"\n",
        "            else:\n",
        "                err += 1\n",
        "                operr[op] += 1\n",
        "                r = \"Error\"\n",
        "            if verbose is True:\n",
        "                if self.model_is_boolean[op] is True:\n",
        "                    if res[0]==self.false_vect:\n",
        "                        str_result=\"False\"\n",
        "                    elif res[0]==self.true_vect:\n",
        "                        str_result=\"True\"\n",
        "                    else:\n",
        "                        str_result=\"undefined\"\n",
        "                else:\n",
        "                    str_result=res[0]\n",
        "                if res[0]==z:\n",
        "                    print(f\"{s} == {str_result}: {r}\")\n",
        "                else:\n",
        "                    print(f\"{s} != {str_result}: {r}\")\n",
        "                    if self.model_is_boolean[op] is False:\n",
        "                        print(bin(res[0]))\n",
        "                        print(bin(z))\n",
        "        opsum = ok+err\n",
        "        if opsum == 0:\n",
        "            opsum = 1\n",
        "        print(f\"Ok: {ok}, Error: {err} -> {ok/opsum*100.0}%\")\n",
        "        print(\"\")\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            opsum = opok[i]+operr[i]\n",
        "            if opsum == 0:\n",
        "                continue\n",
        "            # modify the distribution of training-data generated to favour\n",
        "            # ops with bad test results, so that more training data is\n",
        "            # generated on difficult cases:\n",
        "            self.model_dis[i] = int(operr[i]/opsum*100)+10\n",
        "            print(\n",
        "                f\"OP{self.model_ops[i]}: Ok: {opok[i]}, Error: {operr[i]}\", end=\"\")\n",
        "            print(f\" -> {opok[i]/opsum*100.0}%\")\n",
        "        if valid_ops == None:\n",
        "            print(\"Change probability for ops in new training data:\")\n",
        "            print(f\"Ops:     {self.model_ops}\")\n",
        "            print(f\"Weights: {self.model_dis}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI1RmfIrTJqj"
      },
      "source": [
        "def get_datasets(pre_weight=True, samples=100000, validation_samples=10000, batch_size=2000, short_math=False, valid_ops=None, cache_file_stub='cache', use_cache=True, regenerate_cached_data=False):\n",
        "    math_data = ALU_Dataset(pre_weight=pre_weight)\n",
        "    train = math_data.create_dataset(samples=samples, batch_size=batch_size, short_math=short_math, valid_ops=valid_ops,\n",
        "                                     name=\"training-data\",cache_file=cache_file_stub+\"_train\", use_cache=use_cache, regenerate_cached_data=regenerate_cached_data)\n",
        "    val = math_data.create_dataset(samples=validation_samples, batch_size=batch_size, short_math=short_math, valid_ops=valid_ops,\n",
        "                                   name=\"validation-data\",cache_file=cache_file_stub+\"_val\", use_cache=use_cache, regenerate_cached_data=regenerate_cached_data)\n",
        "    return math_data, train, val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBVSHr_TJqk"
      },
      "source": [
        "## Different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDn-J3Rg4Fm9"
      },
      "source": [
        "def model_large(inputs, regu1=1e-7, regu2=1e-7, regu3=1e-7, neurons=1024, filters=16, strides=2, kernel_size=3):  # neurons must be divisible by 4 for the residual below\n",
        "    # Input goes parallel into 3 streams which will be combined at the end:\n",
        "    # Stream 1: convolutions\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)  # x0)\n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x1 = d1(rinp)\n",
        "    filters=filters*2\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x2 = d2(x1)\n",
        "    filters=filters*2\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    x3 = d3(x2)\n",
        "    filters=filters*2\n",
        "    d4 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xcvl = d4(x3)\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(xcvl)\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    # Stream 2: simple dense layers\n",
        "    df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf1 = df1(inputs)\n",
        "    df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf2 = df2(xf1)\n",
        "    df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xf3 = df3(xf2)\n",
        "\n",
        "    # Stream3: dense layers with residual pathway\n",
        "    dfa1 = layers.Dense(neurons/4, kernel_regularizer=regularizers.l2(\n",
        "        regu3), activation=\"relu\")\n",
        "    xfa1 = dfa1(inputs)\n",
        "    dfa2 = layers.Dense(neurons/4, kernel_regularizer=regularizers.l2(\n",
        "        regu3), activation=\"relu\")\n",
        "    con0 = layers.Concatenate()\n",
        "    xfa2 = con0([dfa2(xfa1),xfa1])\n",
        "    dfa3 = layers.Dense(neurons/2, kernel_regularizer=regularizers.l2(regu3), activation=\"relu\")\n",
        "    con1 = layers.Concatenate()\n",
        "    xfa3 = con1([dfa3(xfa2),xfa2])\n",
        "\n",
        "    # Concat of stream1,2,3\n",
        "    con = layers.Concatenate()\n",
        "    xcon = con([xe1, xf3, xfa3])\n",
        "    dc1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xc1 = dc1(xcon)\n",
        "\n",
        "    # Use sigmoid to map to bits 0..1\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xc1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F467-21ZmX23"
      },
      "source": [
        "def model_medium(inputs, regu1=1e-7, regu2=1e-7, neurons=256, lstm_neurons=128, filters=64, kernel_size=3, strides=2):\n",
        "    #df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "    #    regu1), activation=\"relu\")\n",
        "    #xf1 = df1(inputs)\n",
        "\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs) # xf1)\n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x1 = d1(rinp)\n",
        "    filters = 2*filters\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x2 = d2(x1)\n",
        "    filters = 2*filters\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x3 = d3(x2)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(x3)\n",
        "\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr1 = r1(rinp)\n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr2 = r2(xr1)\n",
        "    r3 = layers.LSTM(lstm_neurons)\n",
        "    xr3 = r3(xr2)\n",
        "\n",
        "    cc = layers.Concatenate()\n",
        "    xc = cc([xf, xr3])\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    xe1 = de1(xc)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEvj-2H2983D"
      },
      "source": [
        "def model_minimal(inputs, neurons=64, regu1=1e-7):\n",
        "    df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf1 = df1(inputs)\n",
        "    df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf2 = df2(xf1)\n",
        "    df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xf3 = df3(xf2)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xf3)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrpnkaSWmSt0"
      },
      "source": [
        "def model_minimal_recurrent(inputs, neurons=64, lstm_neurons=128, regu1=1e-7):\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr1 = r1(rinp)\n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr2 = r2(xr1)\n",
        "    r3 = layers.LSTM(lstm_neurons, return_sequences=True)\n",
        "    xr3 = r3(xr2)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(xr3)\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WDIDkeMfg44"
      },
      "source": [
        "def model_conv1d_recurrent(inputs, neurons=512, lstm_neurons=386, filters=128, kernel_size=3, strides=2, regu0=1e-7, regu1=1e-7, regu2=1e-7):\n",
        "    shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "    rinp = shaper(inputs)\n",
        "\n",
        "    r1 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr1 = r1(rinp)\n",
        "    \n",
        "    d1 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(\n",
        "        regu2), activation=\"relu\")\n",
        "    x1 = d1(xr1)\n",
        "    \n",
        "    r2 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr2 = r2(x1)\n",
        "\n",
        "    d2 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(regu2), activation=\"relu\")\n",
        "    x2 = d2(xr2)\n",
        "\n",
        "    r3 = layers.LSTM(lstm_neurons, return_sequences=True, kernel_regularizer=regularizers.l2(\n",
        "        regu0), recurrent_regularizer=regularizers.l2(\n",
        "        regu0))\n",
        "    xr3 = r3(x2)\n",
        "\n",
        "    d3 = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_regularizer=regularizers.l2(regu2), activation=\"relu\")\n",
        "    x3 = d3(xr3)\n",
        "\n",
        "    flatter = layers.Flatten()\n",
        "    xf = flatter(x3)\n",
        "\n",
        "    de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "        regu1), activation=\"relu\")\n",
        "    xe1 = de1(xf)\n",
        "\n",
        "    de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "    outputs = de2(xe1)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSdpsHyfti96"
      },
      "source": [
        "model_dict = {\"large\": model_large,\n",
        "              \"medium\": model_medium,\n",
        "              \"minimal\": model_minimal,\n",
        "              \"minimal_recurrent\": model_minimal_recurrent,\n",
        "              \"conv_recurrent\": model_conv1d_recurrent\n",
        "              }\n",
        "\n",
        "def create_load_model(save_path=None, model_type='large'):\n",
        "    \"\"\" Create or load a model \"\"\"\n",
        "    if save_path is None or not os.path.exists(save_path): #or is_tpu is True:\n",
        "        print(\"Initializing new model...\")\n",
        "        inputs = keras.Input(shape=(36,))  # depends on encoding of op-code!\n",
        "        if model_type not in model_dict:\n",
        "            print('Unkown model type')\n",
        "            return None\n",
        "        outputs = model_dict[model_type](inputs)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths_\"+model_type)\n",
        "        print(f\"Compiling new model of type {model_type}\")\n",
        "        if use_keras_project is False: \n",
        "            opti = keras.optimizers.Adam(learning_rate=0.001)\n",
        "        else:\n",
        "            opti = optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "    else:\n",
        "        print(f\"Loading standard-format model of type {model_type} from {model_file}\")\n",
        "        model = tf.keras.models.load_model(model_file)\n",
        "        print(\"Continuing training from existing model\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_model(save_path=None, on_tpu=False, model_type='large', import_weights=False):\n",
        "    if is_tpu is True and on_tpu is True:\n",
        "        tpu_is_init=False\n",
        "        if tpu_is_init is False:\n",
        "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
        "            tpu_is_init=True\n",
        "        with tpu_strategy.scope():\n",
        "            print(\"Creating TPU-scope model\")\n",
        "            model=create_load_model(save_path=None, model_type=model_type)\n",
        "        if weights_file is not None and os.path.exists(weights_file):\n",
        "            print(\"Injecting saved weights into TPU model, loading...\")\n",
        "            temp_model = create_load_model(save_path=None, model_type=model_type)\n",
        "            temp_model.load_weights(weights_file)\n",
        "            print(\"Injecting...\")\n",
        "            model.set_weights(temp_model.get_weights())\n",
        "            print(\"Updated TPU weights from saved model\")\n",
        "        return model\n",
        "    else:\n",
        "        print(\"Creating standard-scope model\")\n",
        "        model = create_load_model(save_path=save_path, model_type=model_type)\n",
        "        if import_weights is True and weights_file is not None and os.path.exists(weights_file):\n",
        "            print(\"Injecting saved weights into model, loading...\")        \n",
        "            model.load_weights(weights_file)\n",
        "            imported_weights_file = weights_file+'-imported'\n",
        "            os.rename(weights_file, imported_weights_file)\n",
        "            print(f\"Renamed weights file {weights_file} to {imported_weights_file} to prevent further imports!\")\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZ9m9tbtnoO"
      },
      "source": [
        "def math_train(model, dataset, validation, batch_size=8192, epochs=5000, steps_per_epoch=2000, log_path=\"./logs\"):\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    interrupted = 2\n",
        "    tensorboard_callback = callbacks.TensorBoard(\n",
        "        log_dir=log_path\n",
        "        # histogram_freq=1,\n",
        "        # write_images=1,\n",
        "        # update_freq='batch'\n",
        "        )\n",
        "    try:\n",
        "        if is_tpu:\n",
        "            model.fit(dataset, validation_data=validation, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[tensorboard_callback])\n",
        "            interrupted=0\n",
        "        else:\n",
        "            model.fit(dataset, validation_data=validation, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[tensorboard_callback])\n",
        "            interrupted=0\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"---------INTERRUPT----------\")\n",
        "        print(\"\")\n",
        "        print(\"Training interrupted\")\n",
        "        interrupted = 1 # user stopped runtime\n",
        "    except Exception as e:\n",
        "        interruped = 2  # Bad: something crashed.\n",
        "        print(f\"INTERNAL ERROR\")\n",
        "        print(f\"Exception {e}\")\n",
        "    finally:\n",
        "        return interrupted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWev5OdEdpC"
      },
      "source": [
        "def instantiate_models(model_file, model_type, import_weights=True):\n",
        "    if is_tpu:\n",
        "        # Generate a second CPU model for testing:\n",
        "        test_model = get_model(save_path=None, on_tpu=False, model_type=model_type)\n",
        "        math_model = get_model(save_path=model_file, on_tpu=True, model_type=model_type)\n",
        "    else:\n",
        "        test_model = None\n",
        "        math_model = get_model(save_path=model_file, on_tpu=False, model_type=model_type, import_weights=import_weights)\n",
        "    return math_model, test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn6F0aI_tsXZ"
      },
      "source": [
        "def do_training(math_model, training_dataset, validation_dataset, math_data, epochs_per_cycle, model_file=None, \n",
        "                weights_file=None, test_model=None, cycles=100, steps_per_epoch=1000, reweight_size=1000, valid_ops=None, regenerate_data_after_cycles=0, data_func=None\n",
        "                log_path='./logs'):\n",
        "    # Training\n",
        "    for mep in range(0, cycles):\n",
        "        print()\n",
        "        print()\n",
        "        print(f\"------ Meta-Epoch {mep+1}/{cycles} ------\")\n",
        "        print()\n",
        "        if regenerate_data_after_cycles!=0 and data_func is not None:\n",
        "            if mep>0 and (mep+1)%regenerate_data_after_cycles==0:\n",
        "                math_data, training_dataset, validation_dataset = data_func()\n",
        "        if mep==0 and is_tpu is True:\n",
        "            print(\"There will be some warnings by Tensorflow, documenting some state of internal decoherence, currently they can be ignored.\")\n",
        "        interrupted = math_train(math_model, training_dataset, validation=validation_dataset, epochs=epochs_per_cycle, steps_per_epoch=steps_per_epoch, log_path=log_path)\n",
        "        if interrupted <2:\n",
        "            if is_tpu:\n",
        "                if test_model is None:\n",
        "                    print(\"Fatal: tpu-mode needs test_model on CPU\")\n",
        "                    return False\n",
        "                print(\"Injecting weights into test_model:\")\n",
        "                test_model.set_weights(math_model.get_weights())\n",
        "                if weights_file is not None:\n",
        "                    print(f\"Saving test-model weights to {weights_file}\")\n",
        "                    test_model.save_weights(weights_file)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(test_model, samples=reweight_size, short_math=False, valid_ops=valid_ops, verbose=False)\n",
        "            else:\n",
        "                if model_file is not None:\n",
        "                    print(\"Saving math-model\")\n",
        "                    math_model.save(model_file)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(math_model, samples=reweight_size, short_math=False, valid_ops=valid_ops, verbose=False)\n",
        "        if interrupted>0:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTW4ZQaQTJql"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zha-WkJDTJql"
      },
      "source": [
        "model_type='conv_recurrent'  # see: model_dict definition.\n",
        "epochs_per_cycle=250\n",
        "cycles = 100  # perform 100 cycles, each cycle trains with epochs_per_cycle epochs.\n",
        "regenerate_data_after_cycles=2  # if !=0, the training data will be created anew after each number of \n",
        "                                # regenerace_data_after_cycles cycles. Disadvantage: when training TPU, \n",
        "                                # Google might use the time it takes to regenerate to training data to \n",
        "                                # terminate your session :-/\n",
        "samples=2000000  # Number training data examples\n",
        "batch_size=20000 \n",
        "valid_ops=None  # Default: None (all ops), or list of ops, e.g. ['*', '/'] trains only multiplication and division.\n",
        "steps_per_epoch=samples//batch_size  # TPU stuff\n",
        "model_file, weights_file, cache_stub, log_path = init_paths(model_type=model_type)\n",
        "\n",
        "def train_data(regen=True):\n",
        "    return get_datasets(pre_weight=True, samples=samples, validation_samples=50000, batch_size=batch_size, short_math=False, \n",
        "                                     valid_ops=valid_ops, cache_file_stub=cache_stub, use_cache=True, regenerate_cached_data=regen)\n",
        "math_data, train, val = train_data(regen=False)\n",
        "math_model, test_model = instantiate_models(model_file, model_type, import_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfQ804mzTJql"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2sNM2TTJqm"
      },
      "source": [
        "do_training(math_model, train, val, math_data, epochs_per_cycle, model_file=model_file, \n",
        "            weights_file=weights_file, test_model=test_model, cycles=cycles, steps_per_epoch=steps_per_epoch, valid_ops=valid_ops, \n",
        "            regenerate_data_after_cycles=regenerate_data_after_cycles, data_func=train_data, log_path=log_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO6dRCu6TJqm"
      },
      "source": [
        "# Testing and applying the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcGr9h_p9_R"
      },
      "source": [
        "if is_tpu is False:\n",
        "    test_model = math_model\n",
        "math_data.check_results(test_model, samples=1000, short_math=False, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv8ZRa8GpThA"
      },
      "source": [
        "dx,dy,_,_,_=math_data.create_data_point(22,33,'+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZt0CbNdqpqW"
      },
      "source": [
        "math_data.decode_results(test_model.predict(np.array([dx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV52DL3gq0rI"
      },
      "source": [
        "def calc(inp):\n",
        "    args=inp.split(' ')\n",
        "    if len(args)!=3:\n",
        "        print(\"need three space separated tokens: <int> <operator> <int>, e.g. '3 + 4' or '4 XOR 5'\")\n",
        "        return False\n",
        "    if args[1] not in math_data.model_ops:\n",
        "        print(f\"{args[1]} is not a known operator.\")\n",
        "        return False\n",
        "    op1=int(args[0])\n",
        "    op2=int(args[2])\n",
        "    dx,dy,_,_,_=math_data.create_data_point(op1, op2, args[1])\n",
        "    ans=math_data.decode_results(test_model.predict(np.array([dx])))\n",
        "    print(f\"{op1} {args[1]} {op2} = {ans[0]}\")\n",
        "    op=f\"{op1} {args[1]} {op2}\"\n",
        "    op=op.replace('AND', '&').replace('XOR','^').replace('=','==').replace('OR','|')\n",
        "    an2=eval(op)\n",
        "    if ans[0]!=an2:\n",
        "        print(\"Error\")\n",
        "        print(bin(ans[0]))\n",
        "        print(bin(an2))\n",
        "    return ans[0],an2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqeW9hlrEEE"
      },
      "source": [
        "calc(\"222 = 223\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqb6nBWLTJqm"
      },
      "source": [
        "eval(\"2333+1120\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jjSQodrH0s"
      },
      "source": [
        "calc(\"8812 = 8812\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss8eHRmXs4cE"
      },
      "source": [
        "999/27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoK-LUr-s9IO"
      },
      "source": [
        "calc(\"3 * 4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQNAv4Fs-_w"
      },
      "source": [
        "calc (\"1 AND 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIXKkzgNtBiG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}