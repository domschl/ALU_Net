{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALU_Net.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNjpVLf2486fqX9yYHnH6SH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XijwVpR4s0sQ"
      },
      "source": [
        "\"\"\" A neural net that tries to become an ALU (arithmetic logic unit) \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv54Aqp9s_gs"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, regularizers, callbacks\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoLypnLqxL4p"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try: # Colab instance?\n",
        "    from google.colab import drive\n",
        "    is_colab = True\n",
        "except: # Not? ignore.\n",
        "    is_colab = False\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfh0Gsn4ISLY"
      },
      "source": [
        "# Hardware check:\n",
        "\n",
        "is_tpu = False\n",
        "is_gpu = False\n",
        "tpu_is_init = False\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hw_list=tf.config.experimental.list_logical_devices(hw)\n",
        "    if len(hw_list)>0:\n",
        "        if hw=='TPU':\n",
        "            is_tpu=True\n",
        "        if hw=='GPU':\n",
        "            is_gpu=True\n",
        "    print(f\"{hw}: {hw_list}\") \n",
        "\n",
        "if is_colab:\n",
        "    if not is_tpu:\n",
        "        try:\n",
        "            TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "            tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "            is_tpu = True\n",
        "            print(f\"TPU available at {TPU_ADDRESS}\")\n",
        "        except:\n",
        "            print(\"No TPU available\")\n",
        "    else:\n",
        "        print(f\"TPU available, already connected to {TPU_ADDRESS}\")\n",
        "\n",
        "if not is_tpu:\n",
        "    if not is_gpu:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "    else:\n",
        "        print(\"GPU available\")\n",
        "else:\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    print(\"TPU: eager execution disabled!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0TucwitE4T"
      },
      "source": [
        "class GenSamplesALU():\n",
        "    \"\"\" Generate training data for all ALU operations \"\"\"\n",
        "    # The ALU takes two integers and applies one of the supported\n",
        "    # model_ops. Eg op1=123, op2=100, op='-' -> result 23\n",
        "    # The net is supposed to learn to 'calculate' the results for\n",
        "    # arbitrary op1, op2 (positive integers, 0..32767) and \n",
        "    # the twelve supported ops \n",
        "\n",
        "    def __init__(self):\n",
        "        self.model_ops = [\"+\", \"-\", \"*\", \"/\", \"%\",\n",
        "                          \"AND\", \"OR\", \"XOR\", \">\", \"<\", \"=\", \"!=\"]\n",
        "        # Probabilites for creating a sample for each of the ops, (Will be\n",
        "        # reweighted on checks to generate for samples for 'difficult' ops):\n",
        "        self.model_dis = [10, 10, 10, 10, 10, 10,   10,  10,   10, 10, 10, 10]\n",
        "        self.model_funcs = [self.add_smpl, self.diff_smpl, self.mult_smpl,\n",
        "                            self.div_smpl, self.mod_smpl, self.and_smpl,\n",
        "                            self.bor_smpl, self.xor_smpl, self.greater_smpl,\n",
        "                            self.lesser_smpl, self.eq_smpl, self.neq_smpl]\n",
        "        self.bit_count = 15\n",
        "        self.all_bits_one = 0x7fffffff\n",
        "        self.true_vect = self.all_bits_one\n",
        "        self.false_vect = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def int_to_binary_vect(num_int, num_bits=8):\n",
        "        \"\"\" get a binary encoded vector of n of bit-lenght nm \"\"\"\n",
        "        num_vect = np.zeros(num_bits, dtype=np.float32)\n",
        "        for i in range(0, num_bits):\n",
        "            if num_int & (2**i) != 0:\n",
        "                num_vect[i] = 1.0\n",
        "        return num_vect\n",
        "\n",
        "    @staticmethod\n",
        "    def get_random_bits(bits):\n",
        "        \"\"\" get bits random int 0...2**bits-1 \"\"\"\n",
        "        return random.randint(0, 2**bits-1)\n",
        "\n",
        "    def op_string_to_index(self, op_string):\n",
        "        \"\"\" transform op_string (e.g. '+' -> 0) into corresponding index \"\"\"\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            if self.model_ops[i] == op_string:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def get_data_point(self, equal_distrib=False, short_math=False):\n",
        "        \"\"\" Get a random example for on ALU operation for training \"\"\"\n",
        "        result = -1\n",
        "        op1 = self.get_random_bits(self.bit_count)\n",
        "        op2 = self.get_random_bits(self.bit_count)\n",
        "        if equal_distrib:\n",
        "            op_index = random.randint(0, len(self.model_ops)-1)\n",
        "        else: # make 'difficult' ops more present in training samples:\n",
        "            rx = 0\n",
        "            for md in self.model_dis:\n",
        "                rx += md\n",
        "            rrx = random.randint(0, rx)\n",
        "            rx = 0\n",
        "            op_index = 0\n",
        "            for op_index in range(0, len(self.model_ops)):\n",
        "                rx += self.model_dis[op_index]\n",
        "                if rx > rrx:\n",
        "                    break\n",
        "        return self.encode_op(op1, op2, op_index, short_math)\n",
        "\n",
        "    def encode_op(self, op1, op2, op_index, short_math=False):\n",
        "        \"\"\" turn two ints and operation into training data \"\"\"\n",
        "        result = self.model_funcs[op_index](op1, op2, short_math)\n",
        "        sym = f\"{op1}{self.model_ops[op_index]}{op2}={result}\"\n",
        "        inp = np.concatenate(\n",
        "            [self.int_to_binary_vect(op1, num_bits=16),\n",
        "             self.int_to_binary_vect(op_index, num_bits=4),\n",
        "             self.int_to_binary_vect(op2, num_bits=16)])\n",
        "        oup = self.int_to_binary_vect(result, num_bits=32)\n",
        "        return inp, oup, result, op_index, sym\n",
        "\n",
        "    @staticmethod\n",
        "    def add_smpl(op1, op2, _):\n",
        "        \"\"\" addition training example \"\"\"\n",
        "        result = op1+op2\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def diff_smpl(op1, op2, _):\n",
        "        \"\"\" subtraction training example \"\"\"\n",
        "        if op2 > op1:\n",
        "            op2, op1 = op1, op2\n",
        "        result = op1-op2\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def mult_smpl(op1, op2, short_math=False):\n",
        "        \"\"\" multiplication training example \"\"\"\n",
        "        if short_math:\n",
        "            op1 = op1 % 1000\n",
        "            op2 = op2 % 1000\n",
        "        result = op1*op2\n",
        "        return result\n",
        "\n",
        "    def div_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer division training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1//op2\n",
        "        return result\n",
        "\n",
        "    def mod_smpl(self, op1, op2, _):\n",
        "        \"\"\" modulo (remainder) training example \"\"\"\n",
        "        while op2 == 0:\n",
        "            op2 = self.get_random_bits(self.bit_count)\n",
        "        if op1 < op2 and random.randint(0, 2) != 0:\n",
        "            if op1 != 0:\n",
        "                op1, op2 = op2, op1\n",
        "        result = op1 % op2\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def and_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise AND training example \"\"\"\n",
        "        result = op1 & op2\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def bor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise OR training example \"\"\"\n",
        "        result = op1 | op2\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def xor_smpl(op1, op2, _):\n",
        "        \"\"\" bitwise XOR training example \"\"\"\n",
        "        result = op1 ^ op2\n",
        "        return result\n",
        "\n",
        "    def greater_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation > training example \"\"\"\n",
        "        if op1 > op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return result\n",
        "\n",
        "    def lesser_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation < training example \"\"\"\n",
        "        if op1 < op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return result\n",
        "\n",
        "    def eq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation == training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 == op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return result\n",
        "\n",
        "    def neq_smpl(self, op1, op2, _):\n",
        "        \"\"\" integer comparisation != training example \"\"\"\n",
        "        if random.randint(0, 1) == 0:  # create more cases\n",
        "            op2 = op1\n",
        "        if op1 != op2:\n",
        "            result = self.true_vect\n",
        "        else:\n",
        "            result = self.false_vect\n",
        "        return result\n",
        "\n",
        "    def create_data_point(self, op1, op2, op_string):\n",
        "        \"\"\" create training data from given ints op1, op2 and op_string \"\"\"\n",
        "        op_index = self.op_string_to_index(op_string)\n",
        "        if op_index == -1:\n",
        "            print(f\"Invalid operation {op_string}\")\n",
        "            return np.array([]), np.array([]), -1, -1, None\n",
        "        return self.encode_op(op1, op2, op_index)\n",
        "\n",
        "    def create_training_data(self, samples=10000, short_math=False):\n",
        "        \"\"\" create a number of training samples \"\"\"\n",
        "        x, y, _, _, _ = self.get_data_point()\n",
        "        dpx = np.zeros((samples, len(x)), dtype=np.float32)\n",
        "        dpy = np.zeros((samples, len(y)), dtype=np.float32)\n",
        "        print(f\"Creating {samples} data points (. = 1000 progress)\")\n",
        "        for i in range(0, samples):\n",
        "            if i%100000 == 0:\n",
        "                print(f\"{i:>10} \", end=\"\")\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\".\", end=\"\")\n",
        "                sys.stdout.flush()\n",
        "                if (i+1) % 100000 == 0:\n",
        "                    print()\n",
        "            x, y, _, _, _ = self.get_data_point(\n",
        "                equal_distrib=False, short_math=short_math)\n",
        "            dpx[i, :] = x\n",
        "            dpy[i, :] = y\n",
        "        print()\n",
        "        return dpx, dpy\n",
        "\n",
        "    def create_dataset(self, samples=10000, batch_size=2000, short_math=False):\n",
        "        x, Y = self.create_training_data(samples=samples, short_math=short_math)\n",
        "        shuffle_buffer=10000\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((x, Y)).cache()\n",
        "        dataset=dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
        "        dataset=dataset.repeat() # Mandatory for Keras for now\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "        dataset=dataset.prefetch(-1) # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "        return dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def decode_results(result_int_vects):\n",
        "        \"\"\" take an array of 32-float results from neural net and convert to ints \"\"\"\n",
        "        result_vect_ints = []\n",
        "        for vect in result_int_vects:\n",
        "            if (len(vect) != 32):\n",
        "                print(f\"Ignoring unexpected vector of length {len(vect)}\")\n",
        "            else:\n",
        "                int_result = 0\n",
        "                for i in range(0, 32):\n",
        "                    if vect[i] > 0.5:\n",
        "                        int_result += 2**i\n",
        "                result_vect_ints.append(int_result)\n",
        "        return result_vect_ints\n",
        "\n",
        "    def check_results(self, model, samples=1000, short_math=False, verbose=False):\n",
        "        \"\"\" Run a number of tests on trained model \"\"\"\n",
        "        ok = 0\n",
        "        err = 0\n",
        "        operr = [0]*len(self.model_ops)\n",
        "        opok = [0]*len(self.model_ops)\n",
        "        for _ in range(0, samples):\n",
        "            x, _, z, op, s = self.get_data_point(\n",
        "                equal_distrib=True, short_math=short_math)\n",
        "            res = self.decode_results(model.predict(np.array([x])))\n",
        "            if res[0] == z:\n",
        "                ok += 1\n",
        "                opok[op] += 1\n",
        "                r = \"OK\"\n",
        "            else:\n",
        "                err += 1\n",
        "                operr[op] += 1\n",
        "                r = \"Error\"\n",
        "            if verbose is True:\n",
        "                print(f\"{s} == {res[0]}: {r}\")\n",
        "                print(bin(res[0]))\n",
        "                print(bin(z))\n",
        "        opsum = ok+err\n",
        "        if opsum == 0:\n",
        "            opsum = 1\n",
        "        print(f\"Ok: {ok}, Error: {err} -> {ok/opsum*100.0}%\")\n",
        "        print(\"\")\n",
        "        for i in range(0, len(self.model_ops)):\n",
        "            opsum = opok[i]+operr[i]\n",
        "            if opsum == 0:\n",
        "                opsum = 1\n",
        "            # modify the distribution of training-data generated to favour\n",
        "            # ops with bad test results, so that more training data is\n",
        "            # generated on difficult cases:\n",
        "            self.model_dis[i] = int(operr[i]/opsum*100)+10\n",
        "            print(\n",
        "                f\"OP{self.model_ops[i]}: Ok: {opok[i]}, Error: {operr[i]}\", end=\"\")\n",
        "            print(f\" -> {opok[i]/opsum*100.0}%\")\n",
        "        print(\"Change probability for ops in new training data:\")\n",
        "        print(f\"Ops:    {self.model_ops}\")\n",
        "        print(f\"Weight: {self.model_dis}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSdpsHyfti96"
      },
      "source": [
        "def create_load_model(model_file='math_model'):\n",
        "    \"\"\" Create of load a model \"\"\"\n",
        "    if model_file is None or not os.path.exists(model_file) or is_tpu is True:\n",
        "        regu1 = 1e-8\n",
        "        regu2 = 1e-8\n",
        "        neurons = 368\n",
        "        inputs = keras.Input(shape=(36,))  # depends on encoding of op-code!\n",
        "\n",
        "        shaper = layers.Reshape(target_shape=(36, 1,), input_shape=(36,))\n",
        "        rinp = shaper(inputs)  # x0)\n",
        "        d1 = layers.Conv1D(filters=48, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x1 = d1(rinp)\n",
        "        d2 = layers.Conv1D(filters=64, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x2 = d2(x1)\n",
        "        d3 = layers.Conv1D(filters=128, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x3 = d3(x2)\n",
        "        d4 = layers.Conv1D(filters=128, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x4 = d4(x3)\n",
        "        d5 = layers.Conv1D(filters=128, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x5 = d5(x4)\n",
        "        d6 = layers.Conv1D(filters=128, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        x6 = d6(x5)\n",
        "        d7 = layers.Conv1D(filters=128, kernel_size=6, kernel_regularizer=regularizers.l2(\n",
        "            regu1), activation=\"relu\")\n",
        "        xcvl = d7(x6)\n",
        "        flatter = layers.Flatten()\n",
        "        xf = flatter(xcvl)\n",
        "        de1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xe1 = de1(xf)\n",
        "\n",
        "        df1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf1 = df1(inputs)\n",
        "        df2 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf2 = df2(xf1)\n",
        "        df3 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xf3 = df3(xf2)\n",
        "\n",
        "        con = layers.Concatenate()\n",
        "        xcon = con([xe1, xf3])\n",
        "        dc1 = layers.Dense(neurons, kernel_regularizer=regularizers.l2(\n",
        "            regu2), activation=\"relu\")\n",
        "        xc1 = dc1(xcon)\n",
        "\n",
        "        de2 = layers.Dense(32, activation=\"sigmoid\")\n",
        "        outputs = de2(xc1)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths\")\n",
        "        # , metrics=[\"accuracy\"])\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "        print(\"Compiling new model\")\n",
        "        if is_tpu is True:\n",
        "            pass\n",
        "            # if model_file is not None and os.path.exists(model_file):\n",
        "            #    print(\"Injecting saved weights into TPU model, loading...\")\n",
        "            #    temp_model=tf.keras.models.load_model(model_file)\n",
        "            #    print(\"Injecting...\")\n",
        "            #    model.set_weights(temp_model.get_weights())\n",
        "            #    print(\"Updated TPU weights from saved model\")\n",
        "    else:\n",
        "        print(\"Loading standard-format model\")\n",
        "        model = tf.keras.models.load_model(model_file)\n",
        "        print(\"Continuing training from existing model\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_model(model_file='math_model', on_tpu=False):\n",
        "    if is_tpu is True and on_tpu is True:\n",
        "        tpu_is_init=False\n",
        "        if tpu_is_init is False:\n",
        "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "            # tf.config.experimental_connect_to_cluster(cluster_resolver) # eager mode only! not TPU!\n",
        "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
        "            tpu_is_init=True\n",
        "        with tpu_strategy.scope():\n",
        "            print(\"Creating TPU-scope model\")\n",
        "            return create_load_model(model_file=model_file)\n",
        "    else:\n",
        "        print(\"Creating standard-scope model\")\n",
        "        return create_load_model(model_file=model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZ9m9tbtnoO"
      },
      "source": [
        "def math_train(model, dataset, batch_size=8192, epochs=5000, steps_per_epoch=2000):\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    interrupted = False\n",
        "    tensorboard_callback = callbacks.TensorBoard(\n",
        "        log_dir=\"./logs\",\n",
        "        histogram_freq=1,\n",
        "        write_images=1,\n",
        "        update_freq='batch')\n",
        "    try:\n",
        "        model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, # , batch_size=batch_size\n",
        "                  verbose=1, callbacks=[tensorboard_callback])  # validation_split=0.03 (not datasets!)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"---------INTERRUPT----------\")\n",
        "        print(\"\")\n",
        "        print(\"Training interrupted\")\n",
        "        interrupted = True\n",
        "    except Exception as e:\n",
        "        print(f\"Exception {e}\")\n",
        "    finally:\n",
        "        return interrupted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0SfAQMUvGYF"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380NmaHT7FqW"
      },
      "source": [
        "save_model = True\n",
        "if is_colab:\n",
        "    mountpoint='/content/drive'\n",
        "    root_path='/content/drive/My Drive'\n",
        "    if not os.path.exists(root_path):\n",
        "        drive.mount(mountpoint)\n",
        "    if not os.path.exists(root_path):\n",
        "        print(f\"Something went wrong with Google Drive access. Cannot save model to {root_path}\")\n",
        "        save_model = False\n",
        "else:\n",
        "    root_path='.'\n",
        "\n",
        "if save_model:\n",
        "    project_path=os.path.join(root_path,\"Colab Notebooks/ALU_Net\")\n",
        "    model_file=os.path.join(project_path,'math_model')\n",
        "else:\n",
        "    model_file=None\n",
        "print(f\"Saving model to: {model_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DsIyBlFvIr6"
      },
      "source": [
        "BATCH_SIZE = 2000\n",
        "SAMPLES = 500000\n",
        "EPOCHS_PER_MASTER_CYCLE = 50\n",
        "MASTER_CYCLES = 100\n",
        "STEPS_PER_EPOCH = SAMPLES // BATCH_SIZE\n",
        "REWEIGHT_SIZE = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWev5OdEdpC"
      },
      "source": [
        "# Initialize model(s)\n",
        "math_data = GenSamplesALU()\n",
        "if is_tpu:\n",
        "    # Generate a second CPU model for testing:\n",
        "    test_model = get_model(model_file=None, on_tpu=False)\n",
        "math_model = get_model(model_file=model_file, on_tpu=is_tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn6F0aI_tsXZ"
      },
      "source": [
        "# Training\n",
        "for _ in range(0, MASTER_CYCLES):\n",
        "    dataset = math_data.create_dataset(\n",
        "        samples=SAMPLES, batch_size=BATCH_SIZE, short_math=False)\n",
        "    interrupted = math_train(math_model, dataset, epochs=EPOCHS_PER_MASTER_CYCLE, steps_per_epoch=STEPS_PER_EPOCH)\n",
        "    if is_tpu:\n",
        "        print(\"Injecting weights into test_model:\")\n",
        "        test_model.set_weights(math_model.get_weights())\n",
        "        # print(\"Saving test-model\")\n",
        "        # test_model.save(model_file)\n",
        "        print(\"Done\")\n",
        "        math_data.check_results(test_model, samples=REWEIGHT_SIZE, short_math=False, verbose=False)\n",
        "    else:\n",
        "        print(\"Saving math-model\")\n",
        "        math_model.save(model_file)\n",
        "        print(\"Done\")\n",
        "        math_data.check_results(math_model, samples=REWEIGHT_SIZE, short_math=False, verbose=False)\n",
        "    if interrupted:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcGr9h_p9_R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}