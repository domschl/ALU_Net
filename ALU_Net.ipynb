{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALU_Net.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/ALU_Net/blob/main/ALU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XijwVpR4s0sQ"
      },
      "source": [
        "# Simulating an ALU (arithmetic logic unit) with a neural network\n",
        "\n",
        "The neural network is trained to perform the operations `+`, `-`, `*`, `/`, `%`, `AND`, `OR`, `XOR`, `>`, `<`, `=`, `!=` on two unsigned integers and return the result.\n",
        "\n",
        "## This notebook can run\n",
        "\n",
        "- on local jupyter instances with a local graphics card\n",
        "- on Mac M1 with local jupyter instance and [Apple's tensorflow-plugin](https://developer.apple.com/metal/tensorflow-plugin/)\n",
        "- on Google Colab instances with either GPU or TPU runtime. The colab version uses a Google Drive account to cache data and model state within a Google Drive directory `My Drive/Colab Notebooks/ALU_Net`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ui7VokTJqc"
      },
      "source": [
        "## 1. Configuration and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2sNM2TTJqm"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "use_keras_project_versions=False\n",
        "# Namespaces, namespaces\n",
        "if use_keras_project_versions is False:\n",
        "    # print(\"Importing Keras from tensorflow project (it won't work otherwise with TPU)\")\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "else:\n",
        "    # print(\"Importing Keras from keras project (which had recently declared independence [again]) -- as recommended\")\n",
        "    use_keras_project_versions=True\n",
        "    import keras\n",
        "    from keras import layers, regularizers, callbacks, metrics, optimizers\n",
        "\n",
        "try:\n",
        "    # Google Drive is used in Colab instances to save trained nets and tensorboard logs\n",
        "    from google.colab import drive\n",
        "    is_colab_init = True\n",
        "except:\n",
        "    is_colab_init = False\n",
        "    pass\n",
        "\n",
        "if is_colab_init is True:\n",
        "    # The following code loads the utility modules directly from github\n",
        "    # Into Google Colab (or other jupyter instances)\n",
        "    # WARNING: indeterministic caching by infrastructure: it might take some\n",
        "    # minutes until a change in github is actually accessible in colab (aggressive caching). Sometimes\n",
        "    # it's necessary to factory reset the runtime, and even that gets ignoredd from time to time.\n",
        "    def import_from_github(fn, repo_root_link, force_github_update=False):\n",
        "        if os.path.exists(fn) is False or force_github_update is True:\n",
        "            repo_link=repo_root_link+fn\n",
        "            print(f\"Loading {fn} module from github at {repo_link}...\")\n",
        "            if os.path.exists(fn) is True:\n",
        "                !rm -v {fn}\n",
        "            !wget -nv --show-progress {repo_link}\n",
        "    force_github_update = True  # Note: Even if set to True, you still need to restart the runtime to get an updated version.\n",
        "    repo_root_link = 'https://raw.githubusercontent.com/domschl/ALU_Net/main/'\n",
        "    import_from_github('ml_env_tools.py', repo_root_link, force_github_update)\n",
        "    import_from_github('ml_tuner.py', repo_root_link, force_github_update)\n",
        "    import_from_github('ALU_Dataset.py', repo_root_link, force_github_update)\n",
        "    import_from_github('keras_custom_layers.py', repo_root_link, force_github_update)\n",
        "\n",
        "from ml_env_tools import MLEnv\n",
        "from ml_tuner import MLTuner\n",
        "from ALU_Dataset import ALU_Dataset\n",
        "from keras_custom_layers import ResidualBlock, ResidualDense, ResidualDenseStack, ParallelResidualDenseStacks, SelfAttention, MultiHeadSelfAttention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_tOVlPdamx"
      },
      "source": [
        "def model_res_mod(inputs, params):\n",
        "    # see: keras_custom_layers.py for layer definition:\n",
        "    x=inputs\n",
        "    print(f\"input-shape: {x.shape}\")\n",
        "    self_att=[]\n",
        "    for _ in range(0, params['self_attention_layers']):\n",
        "        self_att.append(MultiHeadSelfAttention(params['self_attention_heads']))\n",
        "    for i in range(0, params['self_attention_layers']):\n",
        "        x=self_att[i](x)+x\n",
        "    fl = layers.Flatten()\n",
        "    print(f\"x.shape bef. fl: {x.shape}\")\n",
        "    x = fl(x)\n",
        "    print(f\"x.shape after. fl: {x.shape}\")\n",
        "    scale = layers.Dense(params['units'], activation=None)\n",
        "    x=scale(x)\n",
        "    prds = ResidualDenseStack(params[\"units\"], params[\"layers\"], regularizer=params[\"regularizer\"])\n",
        "    x=prds(x)\n",
        "    rescale = layers.Dense(params['output_size'], activation=\"sigmoid\")\n",
        "    outputs = rescale(x)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn3rAlvqgmWO"
      },
      "source": [
        "def create_load_model(ml_env:MLEnv, model_variant, params, save_path=None, import_weights=True):\n",
        "    \"\"\" Create or load a model \"\"\"\n",
        "    if save_path is None or not os.path.exists(save_path) or import_weights is False: #or is_tpu is True:\n",
        "        print(\"Initializing new model...\")\n",
        "        # inputs = keras.Input(shape=(params['input_size'],))  # depends on encoding of op-code!\n",
        "        inputs = keras.Input(shape=params['input_size'])  # depends on encoding of op-code!\n",
        "        if model_variant not in model_variants:\n",
        "            print('Unkown model type')\n",
        "            return None\n",
        "        outputs = model_variants[model_variant](inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"maths_\"+model_variant)\n",
        "        print(f\"Compiling new model of type {model_variant}\")\n",
        "        if use_keras_project_versions is False: \n",
        "            opti = keras.optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
        "        else:\n",
        "            opti = optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
        "        if ml_env.is_tpu:\n",
        "            # use steps_per_execution magic (or not)\n",
        "            # model.compile(loss=\"mean_squared_error\", optimizer=opti, steps_per_execution=50, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "        else:\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=opti, metrics=[metrics.MeanSquaredError(), 'accuracy'])\n",
        "    else:\n",
        "        print(f\"Loading standard-format model of type {model_variant} from {model_path}\")\n",
        "        model = tf.keras.models.load_model(save_path)\n",
        "        print(\"Continuing training from existing model\")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqxHzunzgmWP"
      },
      "source": [
        "def get_model(ml_env, model_variant, params, save_path=None, on_tpu=False, import_weights=False):\n",
        "    if on_tpu is True:\n",
        "        if ml_env.tpu_is_init is False:\n",
        "            cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=ml_env.tpu_address)\n",
        "            tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "            tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)    \n",
        "            ml_env.tpu_is_init=True\n",
        "        with tpu_strategy.scope():\n",
        "            print(\"Creating TPU-scope model\")\n",
        "            model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
        "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
        "            print(\"Injecting saved weights into TPU model, loading...\")\n",
        "            temp_model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
        "            temp_model.load_weights(ml_env.weights_file)\n",
        "            print(\"Injecting...\")\n",
        "            model.set_weights(temp_model.get_weights())\n",
        "            print(\"Updated TPU weights from saved model\")\n",
        "        return model\n",
        "    else:\n",
        "        print(\"Creating standard-scope model\")\n",
        "        model = create_load_model(ml_env, model_variant, params, save_path=save_path, import_weights=import_weights)\n",
        "        if import_weights is True and ml_env.weights_file is not None and os.path.exists(ml_env.weights_file):\n",
        "            print(\"Injecting saved weights into model, loading...\")        \n",
        "            model.load_weights(ml_env.weights_file)\n",
        "            imported_weights_file = ml_env.weights_file+'-imported'\n",
        "            os.rename(ml_env.weights_file, imported_weights_file)\n",
        "            print(f\"Renamed weights file {ml_env.weights_file} to {imported_weights_file} to prevent further imports!\")\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4HDT65wgmWP"
      },
      "source": [
        "def math_train(mlenv:MLEnv, model, dataset, validation, batch_size=8192, epochs=5000, steps_per_epoch=2000, log_path=\"./logs\"):\n",
        "    \"\"\" Training loop \"\"\"\n",
        "    interrupted = 2\n",
        "    hist = None\n",
        "    tensorboard_callback = callbacks.TensorBoard(\n",
        "        log_dir=log_path\n",
        "        # histogram_freq=1\n",
        "        # update_freq='batch'\n",
        "        )\n",
        "    if mlenv.is_tpu is False: # TPUs update Tensorboard too asynchronously, data is corrupted by updates during mirroring.\n",
        "        lambda_callback = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_end = ml_env.epoch_time_func\n",
        "        )\n",
        "    try:\n",
        "        if ml_env.is_tpu:\n",
        "            if use_validation_with_tpu is True:\n",
        "                hist = model.fit(dataset, validation_data=validation, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1, callbacks=[tensorboard_callback])\n",
        "            else:\n",
        "                hist = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[tensorboard_callback])\n",
        "            interrupted=0\n",
        "        else:\n",
        "            hist = model.fit(dataset, validation_data=validation, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[tensorboard_callback, lambda_callback])\n",
        "            interrupted=0\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"---------INTERRUPT----------\")\n",
        "        print(\"\")\n",
        "        print(\"Training interrupted\")\n",
        "        interrupted = 1 # user stopped runtime\n",
        "    except Exception as e:\n",
        "        interruped = 2  # Bad: something crashed.\n",
        "        print(f\"INTERNAL ERROR\")\n",
        "        print(f\"Exception {e}\")\n",
        "    finally:\n",
        "        return interrupted, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9_Ey8nDgmWP"
      },
      "source": [
        "def instantiate_models(ml_env:MLEnv, model_variant, params, save_path=None, import_weights=True):\n",
        "    if ml_env.is_tpu:\n",
        "        # Generate a second CPU model for testing:\n",
        "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=True, import_weights=import_weights)\n",
        "        test_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
        "    else:\n",
        "        test_model = None\n",
        "        math_model = get_model(ml_env, model_variant, params, save_path=save_path, on_tpu=False, import_weights=import_weights)\n",
        "    return math_model, test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL4L7FZYgmWP"
      },
      "source": [
        "def do_training(mlenv:MLEnv, math_model, training_dataset, validation_dataset, math_data, epochs_per_cycle, model_path=None, \n",
        "                weights_file=None, test_model=None, cycles=100, steps_per_epoch=1000, reweight_size=1000, valid_ops=None, regenerate_data_after_cycles=0, data_func=None,\n",
        "                log_path='./logs'):\n",
        "    # Training\n",
        "    for mep in range(0, cycles):\n",
        "        print()\n",
        "        print()\n",
        "        print(f\"------ Meta-Epoch {mep+1}/{cycles} ------\")\n",
        "        print()\n",
        "        if regenerate_data_after_cycles!=0 and data_func is not None:\n",
        "            if mep>0 and (mep+1)%regenerate_data_after_cycles==0:\n",
        "                training_dataset, validation_dataset = data_func()\n",
        "        if mep==0 and ml_env.is_tpu is True:\n",
        "            print(\"There will be some warnings by Tensorflow, documenting some state of internal decoherence, currently they can be ignored.\")\n",
        "        interrupted, hist = math_train(ml_env, math_model, training_dataset, validation=validation_dataset, epochs=epochs_per_cycle, steps_per_epoch=steps_per_epoch, log_path=log_path)\n",
        "        if interrupted <2:\n",
        "            if ml_env.is_tpu:\n",
        "                mlenv.gdrive_log_mirror()  # TPUs can only savely mirror Tensorboard data once training is finished for an meta-epoch.\n",
        "                if test_model is None:\n",
        "                    print(\"Fatal: tpu-mode needs test_model on CPU\")\n",
        "                    return False\n",
        "                print(\"Injecting weights into test_model:\")\n",
        "                test_model.set_weights(math_model.get_weights())\n",
        "                if weights_file is not None:\n",
        "                    print(f\"Saving test-model weights to {weights_file}\")\n",
        "                    test_model.save_weights(weights_file)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(test_model, samples=reweight_size, vector=vector, valid_ops=valid_ops, verbose=False)\n",
        "            else:\n",
        "                if model_path is not None:\n",
        "                    print(\"Saving math-model\")\n",
        "                    math_model.save(model_path)\n",
        "                    print(\"Done\")\n",
        "                print(f\"Checking {reweight_size} datapoints for accuracy...\")\n",
        "                math_data.check_results(math_model, samples=reweight_size, vector=vector, valid_ops=valid_ops, verbose=False)\n",
        "        if interrupted>0:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b07h1AVAmtI"
      },
      "source": [
        "ml_env=MLEnv()\n",
        "bit_count = 15\n",
        "math_data=ALU_Dataset(ml_env, bit_count = bit_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yjwuceDgmWQ"
      },
      "source": [
        "model_variants = {\"res_mod\": model_res_mod,\n",
        "                  }\n",
        "\n",
        "model_variant = 'res_mod'  # see: model_variants definition.\n",
        "epochs_per_cycle = 100\n",
        "cycles = 100  # perform 100 (meta-)cycles, each cycle trains with epochs_per_cycle epochs.\n",
        "regenerate_data_after_cycles = 0  # if !=0, the training data will be created anew after each number of \n",
        "                                  # regenerace_data_after_cycles cycles. Disadvantage: when training TPU, \n",
        "                                  # Google might use the time it takes to regenerate to training data to \n",
        "                                  # terminate your session :-/\n",
        "samples = 4000000  # Number training data examples. \n",
        "                   # WARNING: TPU simply crashes, if 2GB limit for entire set is reached.\n",
        "                   # Possible solutions: https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#running_the_tfdata_service,\n",
        "                   # https://www.tensorflow.org/api_docs/python/tf/data/experimental/service , https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_input_pipeline.py#L33\n",
        "validation_samples=100000\n",
        "if ml_env.is_tpu is True:\n",
        "    batch_size = 20000\n",
        "else:\n",
        "    batch_size = 2000\n",
        "import_weights=True\n",
        "if import_weights is False:\n",
        "    print(\"WARNING: import weights is set to False!\")\n",
        "valid_ops = None  # Default: None (all ops), or list of ops, e.g. ['*', '/'] trains only multiplication and division.\n",
        "# valid_ops = ['*','/','+','-']\n",
        "# valid_ops = ['*']\n",
        "steps_per_epoch = samples // batch_size  # TPU stuff\n",
        "validation_steps= validation_samples // batch_size  # again TPU only\n",
        "use_validation_with_tpu = False  # Is somehow really, really slow\n",
        "\n",
        "params_res_mod={\n",
        "    \"self_attention_layers\": 6,\n",
        "    \"self_attention_heads\": 12,\n",
        "    \"units\": 256,\n",
        "    \"learning_rate\": 0.002,\n",
        "    \"layers\": 2,\n",
        "    \"regularizer\": 1e-9\n",
        "    }\n",
        "\n",
        "params=params_res_mod\n",
        "vector = True\n",
        "\n",
        "if vector is True:\n",
        "    params['input_size'] = [3, math_data.embedding_size]\n",
        "else:\n",
        "    params['input_size'] = math_data.input_size\n",
        "params['output_size'] = math_data.output_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnBLx9GwgmWQ"
      },
      "source": [
        "root_path, project_path, model_path, weights_file, cache_path, log_path = ml_env.init_paths(\"ALU_Net\", \"math_model\", model_variant=model_variant, log_to_gdrive=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iliMhQoN2UU"
      },
      "source": [
        "apply_model_tuner = False   # Use GPU (not TPU!) for model_tuner."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ZPfXQdPe-d"
      },
      "source": [
        "if apply_model_tuner is True:\n",
        "    as_train, as_val = math_data.get_datasets(samples=500000, validation_samples=50000, vector=vector, cache_path=cache_path)\n",
        "\n",
        "    def tuner_eval(ml_env:MLEnv, model_variant, params, batch_size, epochs):\n",
        "        math_model, _ = instantiate_models(ml_env, model_variant, params, save_path=None, import_weights=False)\n",
        "        interrupted, hist = math_train(ml_env, math_model, as_train, as_val, batch_size=batch_size, epochs=epochs)\n",
        "        print(params, end=\" [ \")\n",
        "        res = math_data.check_results(math_model, samples=100, valid_ops=valid_ops, verbose=False)\n",
        "        ev = 1/hist.history['val_loss'][-1]+hist.history['val_accuracy'][-1]*20\n",
        "        if res>0:\n",
        "            print(\"Success-rate: {res}\")\n",
        "            ev += res*5000\n",
        "        return ev\n",
        "\n",
        "    tuner_eval_func = lambda params : tuner_eval(ml_env, model_variant, params, batch_size=batch_size, epochs=20)\n",
        "    ml_tuner = MLTuner(ml_env, model_variant)\n",
        "\n",
        "    param_space_minimal_prm={\n",
        "    \"dense_layers\": [4,8,12],\n",
        "    \"dense_neurons\":[256,512,768], \n",
        "    \"learning_rate\": [0.001,0.002],\n",
        "    \"regu1\": [1e-8,1e-7]\n",
        "    }\n",
        "\n",
        "    best_params = ml_tuner.tune(param_space_minimal_prm, tuner_eval_func)\n",
        "    params = best_params\n",
        "    import_weights=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCOAdbtPfQki"
      },
      "source": [
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wN1G_qPgmWQ"
      },
      "source": [
        "create_train_val_data = lambda regen : math_data.get_datasets(pre_weight=False, samples=samples, validation_samples=validation_samples, batch_size=batch_size, \n",
        "                                     vector=vector, valid_ops=valid_ops, cache_path=cache_path, use_cache=True, regenerate_cached_data=regen)\n",
        "create_train_val_data_regen = lambda : create_train_val_data(True)\n",
        "train, val = create_train_val_data(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqUfbmi_KZWU"
      },
      "source": [
        "train.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZ_R7anJRda"
      },
      "source": [
        "# !pip install tensorflow_datasets\n",
        "import tensorflow_datasets as tdfs\n",
        "sa=SelfAttention()\n",
        "nval=tdfs.as_numpy(val)\n",
        "for n in nval:\n",
        "    print(n[0].shape)   \n",
        "    print(sa(n[0]).shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSL6lysogmWQ"
      },
      "source": [
        "math_model, test_model = instantiate_models(ml_env, model_variant, params, save_path=model_path, import_weights=import_weights)\n",
        "# math_model, test_model = instantiate_models(ml_env, model_variant, params, save_path=None, import_weights=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epfwj4czgmWQ"
      },
      "source": [
        "try:\n",
        "    # use the python variable log_path:\n",
        "    get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntW3khTWgmWQ"
      },
      "source": [
        "do_training(ml_env, math_model, train, val, math_data, epochs_per_cycle, model_path=model_path, \n",
        "            weights_file=weights_file, test_model=test_model, cycles=cycles, steps_per_epoch=steps_per_epoch, valid_ops=valid_ops, \n",
        "            regenerate_data_after_cycles=regenerate_data_after_cycles, data_func=create_train_val_data_regen, log_path=log_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO6dRCu6TJqm"
      },
      "source": [
        "# Testing and applying the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcGr9h_p9_R",
        "tags": []
      },
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    test_model = math_model\n",
        "math_data.check_results(test_model, samples=100, valid_ops=valid_ops, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv8ZRa8GpThA"
      },
      "source": [
        "dx,dy,_,_,_=math_data.create_data_point(22,33,'*'); print(22*33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZt0CbNdqpqW"
      },
      "source": [
        "r=test_model.predict(np.array([dx]))\n",
        "print(r)\n",
        "math_data.decode_results(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV52DL3gq0rI"
      },
      "source": [
        "def calc(inp):\n",
        "    args=inp.split(' ')\n",
        "    if len(args)!=3:\n",
        "        print(\"need three space separated tokens: <int> <operator> <int>, e.g. '3 + 4' or '4 XOR 5'\")\n",
        "        return False\n",
        "    if args[1] not in math_data.model_ops:\n",
        "        print(f\"{args[1]} is not a known operator.\")\n",
        "        return False\n",
        "    op1=int(args[0])\n",
        "    op2=int(args[2])\n",
        "    dx,dy,_,_,_=math_data.create_data_point(op1, op2, args[1])\n",
        "    ans=math_data.decode_results(test_model.predict(np.array([dx])))\n",
        "    print(f\"{op1} {args[1]} {op2} = {ans[0]}\")\n",
        "    op=f\"{op1} {args[1]} {op2}\"\n",
        "    op=op.replace('AND', '&').replace('XOR','^').replace('=','==').replace('OR','|')\n",
        "    an2=eval(op)\n",
        "    if ans[0]!=an2:\n",
        "        print(\"Error\")\n",
        "        print(bin(ans[0]))\n",
        "        print(bin(an2))\n",
        "    return ans[0],an2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqeW9hlrEEE"
      },
      "source": [
        "calc(\"22 * 33\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jjSQodrH0s"
      },
      "source": [
        "calc(\"1 = 1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoK-LUr-s9IO"
      },
      "source": [
        "calc(\"3 * 4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQNAv4Fs-_w"
      },
      "source": [
        "calc (\"1 AND 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL8L83LllnoZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}